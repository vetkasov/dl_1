{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3MeKai5Xj6eX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwlFrG-Tj6eY"
   },
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W8BLmtZ3j6eZ"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box)\n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`:\n",
    "\n",
    "        output = module.forward(input)\n",
    "\n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule.\n",
    "\n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self,input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "\n",
    "        This includes\n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "\n",
    "        Make sure to both store the data in `output` field and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        # The easiest case:\n",
    "\n",
    "        # self.output = input\n",
    "        # return self.output\n",
    "\n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input.\n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "\n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "\n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        # The easiest case:\n",
    "\n",
    "        # self.gradInput = gradOutput\n",
    "        # return self.gradInput\n",
    "\n",
    "        pass\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKRkIjT8j6eZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb98PPpJj6ea"
   },
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7y2lav4dj6ea"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially.\n",
    "\n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `output`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "\n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "\n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})\n",
    "\n",
    "\n",
    "        Just write a little loop.\n",
    "        \"\"\"\n",
    "        # for i in range():\n",
    "        #     f'y_{i}' = self.modules[i].forward(f'y_{i-1}')\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = input\n",
    "        for module in self.modules:\n",
    "            self.output = module.updateOutput(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "\n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)\n",
    "            gradInput = module[0].backward(input, g_1)\n",
    "\n",
    "\n",
    "        !!!\n",
    "\n",
    "        To ech module you need to provide the input, module saw while forward pass,\n",
    "        it is used while computing gradients.\n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
    "        and NOT `input` to this Sequential module.\n",
    "\n",
    "        !!!\n",
    "\n",
    "        \"\"\"\n",
    "        # Your code goes here. ################################################\n",
    "        outputs = [input]\n",
    "        for module in self.modules:\n",
    "            outputs.append(module.updateOutput(outputs[-1]))\n",
    "        current_grad = gradOutput\n",
    "        for i in range(len(self.modules)-1, -1, -1):\n",
    "            current_grad = self.modules[i].backward(outputs[i], current_grad)\n",
    "    \n",
    "        self.gradInput = current_grad\n",
    "        return self.gradInput\n",
    "\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "\n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "\n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "\n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfXdYfO4j6ea"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuwvBkuNj6ea",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1 (0.2). Linear transform layer\n",
    "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D0uoyqkpj6ea"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation\n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
    "\n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.W.T) + self.b\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.dot(gradOutput, self.W)\n",
    "        return self.gradInput\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW += np.dot(gradOutput.T, input)\n",
    "        self.gradb += np.sum(gradOutput, axis=0)\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "\n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNOnHXZJj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. (0.2) SoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
    "\n",
    "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VIValI0hj6eb"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        e = np.exp(self.output)\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = e/np.sum(e, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        dot_product = np.sum(self.output * gradOutput, axis=1, keepdims=True)\n",
    "        self.gradInput = self.output * (gradOutput - dot_product)\n",
    "        return self.gradInput\n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy3DJjynj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. (0.2) LogSoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
    "\n",
    "The main goal of this layer is to be used in computation of log-likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xo7DRdAJj6eb"
   },
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(LogSoftMax, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = self.output - np.log(np.sum(np.exp(self.output), axis=1, keepdims=True))\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        softmax = np.exp(self.output)\n",
    "        \n",
    "        # Compute sum over gradients (batch-wise)\n",
    "        sum_grad = np.sum(gradOutput, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute gradient\n",
    "        self.gradInput = gradOutput - softmax * sum_grad\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LogSoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP5QdmmPj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. (0.3) Batch normalization\n",
    "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
    "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
    "```\n",
    "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
    "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
    "```\n",
    "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
    "\n",
    "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fGTTDqVgj6eb"
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(Module):\n",
    "    EPS = 1e-3\n",
    "    def __init__(self, alpha = 0.):\n",
    "        super(BatchNormalization, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.moving_mean = None\n",
    "        self.moving_variance = None\n",
    "        self.cache = None\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            batch_mean = np.mean(input, axis=0, keepdims=True)\n",
    "            batch_variance = np.var(input, axis=0, keepdims=True)\n",
    "            self.moving_mean = self.moving_mean*self.alpha + batch_mean*(1 - self.alpha) if self.moving_mean is not None else batch_mean\n",
    "            self.moving_variance = self.moving_variance*self.alpha + batch_variance*(1 - self.alpha) if self.moving_variance is not None else batch_variance\n",
    "            self.output = (input - batch_mean) / np.sqrt(batch_variance+ self.EPS)\n",
    "            self.cache = (batch_mean, batch_variance)\n",
    "        else:\n",
    "            self.output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS) \n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            batch_mean, batch_variance = self.cache\n",
    "            m = input.shape[0]\n",
    "            d_var = np.sum(gradOutput * (input - batch_mean) * -0.5 * (batch_variance + self.EPS)**(-1.5), axis=0, keepdims=True)\n",
    "            d_mean = np.sum(gradOutput * -1 / np.sqrt(batch_variance + self.EPS), axis=0, keepdims=True) + \\\n",
    "                   d_var * np.sum(-2 * (input - batch_mean), axis=0, keepdims=True) / m\n",
    "            # d_mean = np.sum(gradOutput * (-1) / np.sqrt(batch_variance + self.EPS), axis=0) + \\\n",
    "            #        d_var * (-2) * np.mean(input - batch_mean, axis=0)\n",
    "            self.gradInput = gradOutput / np.sqrt(batch_variance + self.EPS) + \\\n",
    "                            d_var * 2 * (input - batch_mean) / m + \\\n",
    "                            d_mean / m\n",
    "        else:\n",
    "            self.gradInput = gradOutput / np.sqrt(self.moving_variance + self.EPS)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"BatchNormalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8XUS3Lt-j6eb"
   },
   "outputs": [],
   "source": [
    "class ChannelwiseScaling(Module):\n",
    "    \"\"\"\n",
    "       Implements linear transform of input y = gamma * x + beta\n",
    "       where gamma, beta - learnable vectors of length x.shape[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_out):\n",
    "        super(ChannelwiseScaling, self).__init__()\n",
    "\n",
    "        stdv = 1./np.sqrt(n_out)\n",
    "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "\n",
    "        self.gradGamma = np.zeros_like(self.gamma)\n",
    "        self.gradBeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = input * self.gamma + self.beta\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * self.gamma\n",
    "        return self.gradInput\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
    "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        self.gradGamma.fill(0)\n",
    "        self.gradBeta.fill(0)\n",
    "\n",
    "    def getParameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        return [self.gradGamma, self.gradBeta]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ChannelwiseScaling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA5zjM3jj6eb"
   },
   "source": [
    "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gackeo1cj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. (0.3) Dropout\n",
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NmLQV3jXj6eb"
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            self.mask = np.random.binomial(size=input.shape, n=1, p=(1 - self.p)).astype('float') / (1 - self.p)\n",
    "            self.output = input * self.mask\n",
    "        else:\n",
    "            self.output = input\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            self.gradInput = gradOutput * self.mask\n",
    "        else:\n",
    "            self.gradInput = gradOutput\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WHGIqJFlhz2"
   },
   "source": [
    "## 6. (2.0) Conv2d\n",
    "Implement [**Conv2d**](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). Use only this list of parameters: (in_channels, out_channels, kernel_size, stride, padding, bias, padding_mode) and fix dilation=1 and groups=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c1RjNoEXlOHP"
   },
   "outputs": [],
   "source": [
    "# class Conv2d(Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size,\n",
    "#                  stride=1, padding=0, bias=True, padding_mode='zeros'):\n",
    "#         super(Conv2d, self).__init__()\n",
    "\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "#         self.bias = bias\n",
    "#         self.padding_mode = padding_mode\n",
    "\n",
    "#     def updateOutput(self, input):\n",
    "#         # Your code goes here. ################################################\n",
    "        \n",
    "#         return  self.output\n",
    "\n",
    "#     def updateGradInput(self, input, gradOutput):\n",
    "#         # Your code goes here. ################################################\n",
    "        \n",
    "#         return self.gradInput\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return \"Conv2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='valid', bias=True, padding_mode='zeros'):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride, stride)\n",
    "        self.padding = padding\n",
    "        self.padding_mode = padding_mode\n",
    "        self.bias = bias\n",
    "\n",
    "        # Инициализация параметров\n",
    "        self.weight = np.random.randn(out_channels, in_channels, *self.kernel_size) * 0.1\n",
    "        if self.bias:\n",
    "            self.bias_params = np.zeros(out_channels)\n",
    "        \n",
    "        # Градиенты\n",
    "        self.gradWeight = np.zeros_like(self.weight)\n",
    "        if self.bias:\n",
    "            self.gradBias = np.zeros_like(self.bias_params)\n",
    "        \n",
    "        self.last_input = None\n",
    "        self.last_padding = ((0, 0), (0, 0))\n",
    "\n",
    "    def _calculate_same_padding(self, in_height, in_width):\n",
    "        out_height = np.ceil(in_height / self.stride[0]).astype(int)\n",
    "        out_width = np.ceil(in_width / self.stride[1]).astype(int)\n",
    "        pad_h = max(0, (out_height - 1) * self.stride[0] + self.kernel_size[0] - in_height)\n",
    "        pad_w = max(0, (out_width - 1) * self.stride[1] + self.kernel_size[1] - in_width)\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "        return ((pad_top, pad_bottom), (pad_left, pad_right))\n",
    "\n",
    "    def _apply_padding(self, input, padding):\n",
    "        if padding == ((0, 0), (0, 0)):\n",
    "            return input\n",
    "        \n",
    "        pad_top, pad_bottom = padding[0]\n",
    "        pad_left, pad_right = padding[1]\n",
    "        \n",
    "        pad_width = (\n",
    "            (0, 0), \n",
    "            (0, 0),\n",
    "            (pad_top, pad_bottom),\n",
    "            (pad_left, pad_right)\n",
    "        )\n",
    "        \n",
    "        if self.padding_mode == 'zeros':\n",
    "            return np.pad(input, pad_width, mode='constant', constant_values=0)\n",
    "        elif self.padding_mode == 'replicate':\n",
    "            return np.pad(input, pad_width, mode='edge')\n",
    "        elif self.padding_mode == 'reflect':\n",
    "            return np.pad(input, pad_width, mode='symmetric')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported padding mode: {self.padding_mode}\")\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        \n",
    "        if self.padding == 'same':\n",
    "            current_padding = self._calculate_same_padding(in_height, in_width)\n",
    "        elif isinstance(self.padding, int):\n",
    "            current_padding = ((self.padding, self.padding), (self.padding, self.padding))\n",
    "        elif isinstance(self.padding, tuple) and len(self.padding) == 2:\n",
    "            if isinstance(self.padding[0], int):\n",
    "                current_padding = ((self.padding[0], self.padding[0]), (self.padding[1], self.padding[1]))\n",
    "            else:\n",
    "                current_padding = self.padding\n",
    "        else:\n",
    "            current_padding = ((0, 0), (0, 0))\n",
    "        \n",
    "        input_padded = self._apply_padding(input, current_padding)\n",
    "        self.last_input = input_padded\n",
    "        self.last_padding = current_padding\n",
    "        out_height = (input_padded.shape[2] - self.kernel_size[0]) // self.stride[0] + 1\n",
    "        out_width = (input_padded.shape[3] - self.kernel_size[1]) // self.stride[1] + 1\n",
    "        self.output = np.zeros((batch_size, self.out_channels, out_height, out_width))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for oh in range(out_height):\n",
    "                    for ow in range(out_width):\n",
    "                        h_start = oh * self.stride[0]\n",
    "                        w_start = ow * self.stride[1]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "                        \n",
    "                        window = input_padded[b, :, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, oc, oh, ow] = np.sum(window * self.weight[oc])\n",
    "                        \n",
    "                        if self.bias:\n",
    "                            self.output[b, oc, oh, ow] += self.bias_params[oc]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        batch_size, _, out_h, out_w = gradOutput.shape\n",
    "        input_padded = self.last_input\n",
    "        grad_input_padded = np.zeros_like(input_padded)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for oh in range(out_h):\n",
    "                    for ow in range(out_w):\n",
    "                        h_start = oh * self.stride[0]\n",
    "                        w_start = ow * self.stride[1]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "                        \n",
    "                        window = input_padded[b, :, h_start:h_end, w_start:w_end]\n",
    "                        self.gradWeight[oc] += window * gradOutput[b, oc, oh, ow]\n",
    "                        grad_input_padded[b, :, h_start:h_end, w_start:w_end] += self.weight[oc] * gradOutput[b, oc, oh, ow]\n",
    "                \n",
    "                if self.bias:\n",
    "                    self.gradBias[oc] += np.sum(gradOutput[b, oc])\n",
    "        \n",
    "        pad_top, pad_bottom = self.last_padding[0]\n",
    "        pad_left, pad_right = self.last_padding[1]\n",
    "        self.gradInput = grad_input_padded[:, :, pad_top:-pad_bottom if pad_bottom !=0 else None, pad_left:-pad_right if pad_right !=0 else None]\n",
    "        \n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Conv2d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = [\n",
    "#             {'batch_size': 8, 'in_channels': 3, 'out_channels': 6, 'height': 32, 'width': 32,\n",
    "#              'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True, 'padding_mode': 'zeros'}\n",
    "#         ]\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# for _ in range(100):\n",
    "#     for params in hyperparams:\n",
    "#         batch_size = params['batch_size']\n",
    "#         in_channels = params['in_channels']\n",
    "#         out_channels = params['out_channels']\n",
    "#         height = params['height']\n",
    "#         width = params['width']\n",
    "#         kernel_size = params['kernel_size']\n",
    "#         stride = params['stride']\n",
    "#         padding = params['padding']\n",
    "#         bias = params['bias']\n",
    "#         padding_mode = params['padding_mode']\n",
    "\n",
    "#         custom_layer = Conv2d(in_channels, out_channels, kernel_size,\n",
    "#                             stride=stride, padding=padding, bias=bias,\n",
    "#                             padding_mode=padding_mode)\n",
    "#         custom_layer.train()\n",
    "\n",
    "#         torch_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "#                                     stride=stride, padding=padding, bias=bias,\n",
    "#                                     padding_mode=padding_mode)\n",
    "\n",
    "#         custom_layer.weight = torch_layer.weight.detach().numpy().copy()\n",
    "# #         if bias:\n",
    "# #             custom_layer.bias = torch_layer.bias.detach().numpy().copy()\n",
    "\n",
    "#         layer_input = np.random.randn(batch_size, in_channels, height, width).astype(np.float32)\n",
    "#         input_var = torch.tensor(layer_input, requires_grad=True)\n",
    "\n",
    "#         custom_output = custom_layer.updateOutput(layer_input)\n",
    "#         torch_output = torch_layer(input_var)\n",
    "#         self.assertTrue(\n",
    "#             np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "#         next_layer_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "#         custom_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "#         torch_output.backward(torch.tensor(next_layer_grad))\n",
    "#         torch_grad = input_var.grad.detach().numpy()\n",
    "#         self.assertTrue(\n",
    "#         np.allclose(torch_grad, custom_grad, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_layer.bias.detach().numpy().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "updUVZE9qixP",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. (0.5) Implement [**MaxPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [**AvgPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html). Use only parameters like kernel_size, stride, padding (negative infinity for maxpool and zero for avgpool) and other parameters fixed as in framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Qys58EzkqhLj"
   },
   "outputs": [],
   "source": [
    "class MaxPool2d(Module):\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super(MaxPool2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
    "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
    "        self.cache = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        batch, num_channels, H_in, W_in = input.shape\n",
    "        H_out = ((H_in + 2*self.padding[0] - self.kernel_size[0])//self.stride[0] + 1)\n",
    "        W_out = ((W_in + 2*self.padding[1] - self.kernel_size[1])//self.stride[1] + 1)\n",
    "        \n",
    "        self.output = np.zeros((batch, num_channels, H_out, W_out))\n",
    "        self.indices = np.zeros((batch, num_channels, H_out, W_out, 2), dtype=int)\n",
    "        padding = np.pad(input, \n",
    "                            ((0,0), (0,0), (self.padding[0],self.padding[0]), (self.padding[1], self.padding[1])), \n",
    "                            mode='constant')\n",
    "        for b in range(batch):\n",
    "            for c in range(num_channels):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start = i * self.stride[0]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_start = j * self.stride[1]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "\n",
    "                        window = padding[b, c, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, c, i, j] = np.max(window)\n",
    "                        max_idx = np.unravel_index(np.argmax(window), window.shape)\n",
    "                        self.indices[b, c, i, j] = [h_start + max_idx[0], w_start + max_idx[1]]\n",
    "        self.cache = (batch, num_channels, H_in, W_in)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        batch, num_channels, H_in, W_in = self.cache\n",
    "        out_shape = (batch, num_channels, \n",
    "                       H_in + 2*self.padding[0], W_in + 2*self.padding[1])\n",
    "        self.gradInput = np.zeros(out_shape)\n",
    "        for b in range(batch):\n",
    "            for c in range(num_channels):\n",
    "                for i in range(gradOutput.shape[2]):\n",
    "                    for j in range(gradOutput.shape[3]):\n",
    "                        h, w = self.indices[b, c, i, j]\n",
    "                        self.gradInput[b, c, h, w] += gradOutput[b, c, i, j]\n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            self.gradInput = self.gradInput[:, :, self.padding[0]:-self.padding[0], self.padding[1]:-self.padding[1]] if self.padding[0] > 0 and self.padding[1] > 0 else \\\n",
    "                           self.gradInput[:, :, self.padding[0]:-self.padding[0], :] if self.padding[0] > 0 else \\\n",
    "                           self.gradInput[:, :, :, self.padding[1]:-self.padding[1]]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MaxPool2d\"\n",
    "\n",
    "class AvgPool2d(Module):\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super(AvgPool2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
    "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
    "        self.cache = None\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        batch, num_channels, H_in, W_in = input.shape\n",
    "        H_out = ((H_in + 2*self.padding[0] - self.kernel_size[0])//self.stride[0] + 1)\n",
    "        W_out = ((W_in + 2*self.padding[1] - self.kernel_size[1])//self.stride[1] + 1)\n",
    "        \n",
    "        self.output = np.zeros((batch, num_channels, H_out, W_out))\n",
    "        self.indices = np.zeros((batch, num_channels, H_out, W_out, 2), dtype=int)\n",
    "        padding = np.pad(input, \n",
    "                            ((0,0), (0,0), (self.padding[0],self.padding[0]), (self.padding[1], self.padding[1])), \n",
    "                            mode='constant')\n",
    "        for b in range(batch):\n",
    "            for c in range(num_channels):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start = i * self.stride[0]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_start = j * self.stride[1]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "\n",
    "                        window = padding[b, c, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, c, i, j] = np.mean(window)\n",
    "        self.cache = (batch, num_channels, H_in, W_in)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        batch, num_channels, H_in, W_in = self.cache\n",
    "        out_shape = (batch, num_channels, \n",
    "                       H_in + 2*self.padding[0], W_in + 2*self.padding[1])\n",
    "        self.gradInput = np.zeros(out_shape)\n",
    "        norm_win = 1 / (self.kernel_size[0] * self.kernel_size[1])\n",
    "        for b in range(batch):\n",
    "            for c in range(num_channels):\n",
    "                for i in range(gradOutput.shape[2]):\n",
    "                    for j in range(gradOutput.shape[3]):\n",
    "                        h_start = i * self.stride[0]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_start = j * self.stride[1]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "                        self.gradInput[b, c, h_start:h_end, w_start:w_end] += gradOutput[b, c, i, j] * norm_win\n",
    "                        \n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            self.gradInput = self.gradInput[:, :, self.padding[0]:-self.padding[0], self.padding[1]:-self.padding[1]] if self.padding[0] > 0 and self.padding[1] > 0 else \\\n",
    "                           self.gradInput[:, :, self.padding[0]:-self.padding[0], :] if self.padding[0] > 0 else \\\n",
    "                           self.gradInput[:, :, :, self.padding[1]:-self.padding[1]]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"AvgPool2d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTN5R3CwrukV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. (0.3) Implement **GlobalMaxPool2d** and **GlobalAvgPool2d**. They do not have testing and parameters are up to you but they must aggregate information within channels. Write test functions for these layers on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYeBQDBhtViy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9. (0.2) Implement [**Flatten**](https://pytorch.org/docs/stable/generated/torch.flatten.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Module):\n",
    "    def __init__(self, start_dim=0, end_dim=-1):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.start_dim = self.start_dim if self.start_dim >= 0 else len(input.shape) + self.start_dim\n",
    "        self.end_dim = self.end_dim if self.end_dim >= 0 else len(input.shape) + self.end_dim\n",
    "        to_shape = list(input.shape[:self.start_dim])\n",
    "        flattened_size = 1\n",
    "        for _ in input.shape[self.start_dim:self.end_dim+1]:\n",
    "            flattened_size *= _\n",
    "        to_shape.append(flattened_size)\n",
    "        to_shape.extend(input.shape[self.end_dim+1:])\n",
    "        self.output = input.reshape(to_shape)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = gradOutput.reshape(input.shape)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Flatten\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o36vPHSSj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_pryRQIj6ec"
   },
   "source": [
    "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sgm8bXjKj6ec"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB0UHGagj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10. (0.1) Leaky ReLU\n",
    "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "agwfkwO0j6ec"
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "\n",
    "        self.slope = slope\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.where(input > 0, input, input * self.slope)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = np.where(input > 0, gradOutput, gradOutput * self.slope)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-STyecvj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 11. (0.1) ELU\n",
    "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jJSzEu1mj6ec"
   },
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        super(ELU, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.where(input > 0, input, self.alpha*(np.exp(input) - 1))\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = np.where(input > 0, gradOutput, self.alpha*(np.exp(input))*gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ELU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn3C7KTqj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 12. (0.1) SoftPlus\n",
    "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xcDPMssrj6ec"
   },
   "outputs": [],
   "source": [
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.log(1 + np.exp(input))\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = gradOutput/(1 + np.exp(-input))\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw3PeZjOuo0e",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 13. (0.2) Gelu\n",
    "Implement [**Gelu**](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SdieE0Dtuo8j"
   },
   "outputs": [],
   "source": [
    "class Gelu(Module):\n",
    "    def __init__(self):\n",
    "        super(Gelu, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        # self.output = 0.5*input*(1 + np.tanh(np.sqrt(2/np.pi) * (input + 0.044715*input**3))) #НЕ РАБОТАЕТ, ПОСКОЛЬКУ В pytorch ИСПОЛЬЗУЕТСЯ ДРУГАЯ АППРОКСИМАЦИЯ -> на тесте не сходится\n",
    "        self.output = 0.5 * input * (1 + torch.erf(torch.from_numpy(input / np.sqrt(2))).numpy())\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = gradOutput * (0.5*(1 + torch.erf(torch.from_numpy(input/np.sqrt(2))).numpy()) + (input * np.exp(-0.5*input**2))/np.sqrt(2*np.pi))\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Gelu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55p7UvPAj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NFaxZaqj6ec"
   },
   "source": [
    "Criterions are used to score the models answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "XGu45A8qj6ec"
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function\n",
    "            associated to the criterion and return the result.\n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result.\n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuU26xkpj6ec"
   },
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- target: **`batch_size x n_feats`**\n",
    "- output: **scalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-i3VNuHhj6ec"
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8LKLWNVj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 14. (0.2) Negative LogLikelihood criterion (numerically unstable)\n",
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
    "- input:   **`batch_size x n_feats`** - probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "die7KvW6j6ec"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterionUnstable(Criterion):\n",
    "    EPS = 1e-15\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterionUnstable, self)\n",
    "        super(ClassNLLCriterionUnstable, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        # return loss of loss function\n",
    "\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = -np.sum(target * np.log(input_clamp)) / target.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        # return gradient of loss function\n",
    "\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = -target / (input_clamp * target.shape[0])\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterionUnstable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHr_JbU5j6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 15. (0.3) Negative LogLikelihood criterion (numerically stable)\n",
    "- input:   **`batch_size x n_feats`** - log probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n",
    "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "v7N8bVP9j6ec"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = -np.sum(target * input) / target.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = -target / target.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-ZnhKxaj6ed"
   },
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC2Bf1PP2Ios"
   },
   "source": [
    "1-я часть задания: реализация слоев, лосей и функций активации - 5 баллов. \\\\\n",
    "2-я часть задания: реализация моделей на своих классах. Что должно быть:\n",
    "  1. Выберите оптимизатор и реализуйте его, чтоб он работал с вами классами. - 1 балл.✅\n",
    "  2. Модель для задачи мультирегрессии на выбраных вами данных. Использовать FCNN, dropout, batchnorm, MSE. Пробуйте различные фукнции активации. Для первой модели попробуйте большую, среднюю и маленькую модель. - 1 балл.✅\n",
    "  3. Модель для задачи мультиклассификации на MNIST. Использовать свёртки, макспулы, флэттэны, софтмаксы - 1 балла.\n",
    "  4. Автоэнкодер для выбранных вами данных. Должен быть на свёртках и полносвязных слоях, дропаутах, батчнормах и тд. - 2 балла. \\\\\n",
    "\n",
    "Дополнительно в оценке каждой модели будет учитываться:\n",
    "1. Наличие правильно выбранной метрики и лосс функции.✅\n",
    "2. Отрисовка графиков лосей и метрик на трейне-валидации. Проверка качества модели на тесте.✅\n",
    "3. Наличие шедулера для lr.✅\n",
    "4. Наличие вормапа.\n",
    "5. Наличие механизма ранней остановки и сохранение лучшей модели.✅\n",
    "6. Свитч лося (метрики) и оптимайзера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, model, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        self.params = self._get_all_parameters()\n",
    "        for param in self.params:\n",
    "            self.m.append(np.zeros_like(param))\n",
    "            self.v.append(np.zeros_like(param))\n",
    "    \n",
    "    def _get_all_parameters(self):\n",
    "        params = []\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        \n",
    "        for module in modules:\n",
    "            if hasattr(module, 'getParameters'):\n",
    "                module_params = module.getParameters()\n",
    "                if module_params:  # Skip empty lists\n",
    "                    if isinstance(module_params[0], (list, tuple, np.ndarray)):\n",
    "                        params.extend(module_params)\n",
    "                    else:\n",
    "                        params.append(module_params)\n",
    "        return params\n",
    "    \n",
    "    def _get_all_gradients(self):\n",
    "        grads = []\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        \n",
    "        for module in modules:\n",
    "            if hasattr(module, 'getGradParameters'):\n",
    "                module_grads = module.getGradParameters()\n",
    "                if module_grads:  # Skip empty lists\n",
    "                    if isinstance(module_grads[0], (list, tuple, np.ndarray)):\n",
    "                        grads.extend(module_grads)\n",
    "                    else:\n",
    "                        grads.append(module_grads)\n",
    "        return grads\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        grads = self._get_all_gradients()\n",
    "        \n",
    "        for i, (param, grad) in enumerate(zip(self.params, grads)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        for module in modules:\n",
    "            if hasattr(module, 'zeroGradParameters'):\n",
    "                module.zeroGradParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.current_lr = initial_lr\n",
    "    \n",
    "    def step(self, epoch=None, metrics=None):\n",
    "        self.update_lr(epoch, metrics)\n",
    "        self.apply_lr()\n",
    "    \n",
    "    def update_lr(self, epoch, metrics):\n",
    "        pass\n",
    "    \n",
    "    def apply_lr(self):\n",
    "        self.optimizer.lr = self.current_lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.current_lr\n",
    "\n",
    "\n",
    "class StepLR(LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, step_size, gamma=0.1):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def update_lr(self, epoch, metrics=None):\n",
    "        if epoch > 0 and epoch % self.step_size == 0:\n",
    "            self.current_lr = self.initial_lr * (self.gamma ** (epoch // self.step_size))\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau(LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, factor=0.1, patience=10, min_lr=1e-6):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.best_metric = None\n",
    "        self.wait = 0\n",
    "    \n",
    "    def update_lr(self, epoch, metrics):\n",
    "        if metrics is None:\n",
    "            return\n",
    "            \n",
    "        if self.best_metric is None:\n",
    "            self.best_metric = metrics\n",
    "        elif metrics > self.best_metric:\n",
    "            self.best_metric = metrics\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                new_lr = max(self.current_lr * self.factor, self.min_lr)\n",
    "                if new_lr < self.current_lr:\n",
    "                    self.current_lr = new_lr\n",
    "                    self.wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSERegressionTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, scheduler=None, patience=5):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.patience = patience\n",
    "        self.train_history = {'loss': [], 'mse': []}\n",
    "        self.val_history = {'loss': [], 'mse': []}\n",
    "        self.best_loss = np.inf\n",
    "        self.best_epoch = 0\n",
    "        self.best_params = None\n",
    "\n",
    "    def _save_best_params(self):\n",
    "        self.best_params = []\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        \n",
    "        for module in modules:\n",
    "            if hasattr(module, 'getParameters'):\n",
    "                params = module.getParameters()\n",
    "                if params:\n",
    "                    self.best_params.append([p.copy() for p in params])\n",
    "\n",
    "    def _load_best_params(self):\n",
    "        if self.best_params is None:\n",
    "            return\n",
    "            \n",
    "        idx = 0\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        \n",
    "        for module in modules:\n",
    "            if hasattr(module, 'getParameters'):\n",
    "                params = module.getParameters()\n",
    "                if params:\n",
    "                    for i in range(len(params)):\n",
    "                        params[i][...] = self.best_params[idx][i]\n",
    "                    idx += 1\n",
    "    \n",
    "    def compute_metrics(self, y_pred, y_true):\n",
    "        mse = np.mean((y_pred - y_true)**2)\n",
    "        return {'mse': mse}\n",
    "    \n",
    "    def train_epoch(self, X, y, batch_size=32, training=True):\n",
    "        epoch_loss = 0\n",
    "        epoch_metrics = {'mse': 0}\n",
    "        num_batches = int(np.ceil(len(X) / batch_size))\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            output = self.model.forward(X_batch)\n",
    "            loss = self.criterion.updateOutput(output, y_batch)\n",
    "            metrics = self.compute_metrics(output, y_batch)\n",
    "            epoch_loss += loss * len(X_batch)\n",
    "            for k in epoch_metrics:\n",
    "                epoch_metrics[k] += metrics[k] * len(X_batch)\n",
    "            if training:\n",
    "                grad_output = self.criterion.updateGradInput(output, y_batch)\n",
    "                self.model.backward(X_batch, grad_output)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "        \n",
    "        return epoch_loss, epoch_metrics\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_metrics = self.train_epoch(X_train, y_train, batch_size, training=True)\n",
    "            self.train_history['loss'].append(train_loss)\n",
    "            self.train_history['mse'].append(train_metrics['mse'])\n",
    "            \n",
    "            val_loss, val_metrics = self.train_epoch(X_val, y_val, batch_size, training=False)\n",
    "            self.val_history['loss'].append(val_loss)\n",
    "            self.val_history['mse'].append(val_metrics['mse'])\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(epoch, -val_loss)\n",
    "            \n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                self._save_best_params()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"LR: {self.optimizer.lr if hasattr(self.optimizer, 'lr') else 'N/A':.6f}\")\n",
    "        \n",
    "        self._load_best_params()\n",
    "        print(f\"Best model restored from epoch {self.best_epoch + 1}\")\n",
    "        \n",
    "        self.plot_history()\n",
    "        return self.train_history, self.val_history\n",
    "    \n",
    "    def plot_history(self):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(np.log(self.train_history['loss']), label='Train Loss')\n",
    "        plt.plot(np.log(self.val_history['loss']), label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss (logarithmized)')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(np.log(self.train_history['mse']), label='Train MSE')\n",
    "        plt.plot(np.log(self.val_history['mse']), label='Validation MSE')\n",
    "        plt.title('Training and Validation MSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE (logarithmized)')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: Train Loss: 197291572.5354 | Val Loss: 861142.1633 | LR: 0.010000\n",
      "Epoch 2/200: Train Loss: 1523223.0619 | Val Loss: 239911.8809 | LR: 0.010000\n",
      "Epoch 3/200: Train Loss: 456644.2299 | Val Loss: 78521.4273 | LR: 0.010000\n",
      "Epoch 4/200: Train Loss: 169907.2232 | Val Loss: 36765.4294 | LR: 0.010000\n",
      "Epoch 5/200: Train Loss: 142787.0983 | Val Loss: 13345.8306 | LR: 0.010000\n",
      "Epoch 6/200: Train Loss: 250563.7334 | Val Loss: 9280.4851 | LR: 0.010000\n",
      "Epoch 7/200: Train Loss: 225912.1296 | Val Loss: 15897.6263 | LR: 0.010000\n",
      "Epoch 8/200: Train Loss: 415258.4840 | Val Loss: 12381.1499 | LR: 0.010000\n",
      "Epoch 9/200: Train Loss: 55184.0734 | Val Loss: 51351.6592 | LR: 0.010000\n",
      "Epoch 10/200: Train Loss: 267509.7382 | Val Loss: 66009.6970 | LR: 0.010000\n",
      "Epoch 11/200: Train Loss: 433837.3771 | Val Loss: 29215.4031 | LR: 0.005000\n",
      "Epoch 12/200: Train Loss: 9048.5597 | Val Loss: 1631.7873 | LR: 0.005000\n",
      "Epoch 13/200: Train Loss: 6160.6736 | Val Loss: 2299.2984 | LR: 0.005000\n",
      "Epoch 14/200: Train Loss: 6142.3611 | Val Loss: 6193.6942 | LR: 0.005000\n",
      "Epoch 15/200: Train Loss: 80763.0644 | Val Loss: 10391.0841 | LR: 0.005000\n",
      "Epoch 16/200: Train Loss: 101927.2959 | Val Loss: 5928.3665 | LR: 0.005000\n",
      "Epoch 17/200: Train Loss: 19481.6576 | Val Loss: 8493.2357 | LR: 0.002500\n",
      "Epoch 18/200: Train Loss: 3056.6350 | Val Loss: 594.9120 | LR: 0.002500\n",
      "Epoch 19/200: Train Loss: 4325.1618 | Val Loss: 957.6348 | LR: 0.002500\n",
      "Epoch 20/200: Train Loss: 15593.6451 | Val Loss: 1359.3300 | LR: 0.002500\n",
      "Epoch 21/200: Train Loss: 17764.8302 | Val Loss: 1404.3254 | LR: 0.002500\n",
      "Epoch 22/200: Train Loss: 16599.9875 | Val Loss: 2441.7384 | LR: 0.002500\n",
      "Epoch 23/200: Train Loss: 27071.2356 | Val Loss: 1971.3435 | LR: 0.001250\n",
      "Epoch 24/200: Train Loss: 1690.7562 | Val Loss: 547.0170 | LR: 0.001250\n",
      "Epoch 25/200: Train Loss: 1773.4333 | Val Loss: 537.5666 | LR: 0.001250\n",
      "Epoch 26/200: Train Loss: 3319.0673 | Val Loss: 1302.2658 | LR: 0.001250\n",
      "Epoch 27/200: Train Loss: 3466.1069 | Val Loss: 1234.8426 | LR: 0.001250\n",
      "Epoch 28/200: Train Loss: 7416.8046 | Val Loss: 1053.4394 | LR: 0.001250\n",
      "Epoch 29/200: Train Loss: 3673.6443 | Val Loss: 2257.9958 | LR: 0.001250\n",
      "Epoch 30/200: Train Loss: 7143.5117 | Val Loss: 1061.4973 | LR: 0.000625\n",
      "Epoch 31/200: Train Loss: 1352.1710 | Val Loss: 458.5077 | LR: 0.000625\n",
      "Epoch 32/200: Train Loss: 1511.9576 | Val Loss: 456.6910 | LR: 0.000625\n",
      "Epoch 33/200: Train Loss: 2330.0705 | Val Loss: 540.3148 | LR: 0.000625\n",
      "Epoch 34/200: Train Loss: 2403.7596 | Val Loss: 735.0317 | LR: 0.000625\n",
      "Epoch 35/200: Train Loss: 2422.0931 | Val Loss: 943.7914 | LR: 0.000625\n",
      "Epoch 36/200: Train Loss: 2463.7041 | Val Loss: 995.1829 | LR: 0.000625\n",
      "Epoch 37/200: Train Loss: 2471.4025 | Val Loss: 1009.9885 | LR: 0.000313\n",
      "Epoch 38/200: Train Loss: 1122.6547 | Val Loss: 387.1678 | LR: 0.000313\n",
      "Epoch 39/200: Train Loss: 1217.4940 | Val Loss: 426.1748 | LR: 0.000313\n",
      "Epoch 40/200: Train Loss: 1312.2863 | Val Loss: 434.8538 | LR: 0.000313\n",
      "Epoch 41/200: Train Loss: 1342.0348 | Val Loss: 454.0682 | LR: 0.000313\n",
      "Epoch 42/200: Train Loss: 1368.4831 | Val Loss: 460.5055 | LR: 0.000313\n",
      "Epoch 43/200: Train Loss: 1384.7703 | Val Loss: 460.9408 | LR: 0.000156\n",
      "Epoch 44/200: Train Loss: 966.1833 | Val Loss: 337.2284 | LR: 0.000156\n",
      "Epoch 45/200: Train Loss: 985.5069 | Val Loss: 350.0325 | LR: 0.000156\n",
      "Epoch 46/200: Train Loss: 1016.3055 | Val Loss: 363.8347 | LR: 0.000156\n",
      "Epoch 47/200: Train Loss: 1034.3335 | Val Loss: 367.2814 | LR: 0.000156\n",
      "Epoch 48/200: Train Loss: 1038.8263 | Val Loss: 367.3436 | LR: 0.000156\n",
      "Epoch 49/200: Train Loss: 1038.8756 | Val Loss: 366.6336 | LR: 0.000078\n",
      "Epoch 50/200: Train Loss: 868.8698 | Val Loss: 309.5399 | LR: 0.000078\n",
      "Epoch 51/200: Train Loss: 877.7270 | Val Loss: 315.4050 | LR: 0.000078\n",
      "Epoch 52/200: Train Loss: 884.4190 | Val Loss: 315.9176 | LR: 0.000078\n",
      "Epoch 53/200: Train Loss: 887.9248 | Val Loss: 315.7500 | LR: 0.000078\n",
      "Epoch 54/200: Train Loss: 889.4161 | Val Loss: 315.2095 | LR: 0.000078\n",
      "Epoch 55/200: Train Loss: 889.7945 | Val Loss: 314.7218 | LR: 0.000039\n",
      "Epoch 56/200: Train Loss: 813.3792 | Val Loss: 277.3132 | LR: 0.000039\n",
      "Epoch 57/200: Train Loss: 814.8545 | Val Loss: 278.1406 | LR: 0.000039\n",
      "Epoch 58/200: Train Loss: 817.3286 | Val Loss: 278.7814 | LR: 0.000039\n",
      "Epoch 59/200: Train Loss: 818.3898 | Val Loss: 279.0508 | LR: 0.000039\n",
      "Epoch 60/200: Train Loss: 818.7115 | Val Loss: 279.0853 | LR: 0.000039\n",
      "Epoch 61/200: Train Loss: 818.6723 | Val Loss: 279.0070 | LR: 0.000020\n",
      "Epoch 62/200: Train Loss: 780.2845 | Val Loss: 267.3003 | LR: 0.000020\n",
      "Epoch 63/200: Train Loss: 780.1702 | Val Loss: 268.1973 | LR: 0.000020\n",
      "Epoch 64/200: Train Loss: 780.8053 | Val Loss: 268.7993 | LR: 0.000020\n",
      "Epoch 65/200: Train Loss: 781.0954 | Val Loss: 269.1147 | LR: 0.000020\n",
      "Epoch 66/200: Train Loss: 781.1767 | Val Loss: 269.2507 | LR: 0.000020\n",
      "Epoch 67/200: Train Loss: 781.1379 | Val Loss: 269.2773 | LR: 0.000010\n",
      "Epoch 68/200: Train Loss: 763.9266 | Val Loss: 262.5155 | LR: 0.000010\n",
      "Epoch 69/200: Train Loss: 763.4301 | Val Loss: 262.5545 | LR: 0.000010\n",
      "Epoch 70/200: Train Loss: 763.5572 | Val Loss: 262.5741 | LR: 0.000010\n",
      "Epoch 71/200: Train Loss: 763.5950 | Val Loss: 262.5671 | LR: 0.000010\n",
      "Epoch 72/200: Train Loss: 763.5737 | Val Loss: 262.5400 | LR: 0.000010\n",
      "Epoch 73/200: Train Loss: 763.5164 | Val Loss: 262.5067 | LR: 0.000010\n",
      "Epoch 74/200: Train Loss: 763.4406 | Val Loss: 262.4640 | LR: 0.000010\n",
      "Epoch 75/200: Train Loss: 763.3542 | Val Loss: 262.4242 | LR: 0.000010\n",
      "Epoch 76/200: Train Loss: 763.2612 | Val Loss: 262.3804 | LR: 0.000010\n",
      "Epoch 77/200: Train Loss: 763.1688 | Val Loss: 262.3341 | LR: 0.000010\n",
      "Epoch 78/200: Train Loss: 763.0725 | Val Loss: 262.2902 | LR: 0.000010\n",
      "Epoch 79/200: Train Loss: 762.9770 | Val Loss: 262.2455 | LR: 0.000010\n",
      "Epoch 80/200: Train Loss: 762.8825 | Val Loss: 262.2003 | LR: 0.000010\n",
      "Epoch 81/200: Train Loss: 762.7884 | Val Loss: 262.1529 | LR: 0.000010\n",
      "Epoch 82/200: Train Loss: 762.6945 | Val Loss: 262.1095 | LR: 0.000010\n",
      "Epoch 83/200: Train Loss: 762.6030 | Val Loss: 262.0640 | LR: 0.000010\n",
      "Epoch 84/200: Train Loss: 762.5090 | Val Loss: 262.0208 | LR: 0.000010\n",
      "Epoch 85/200: Train Loss: 762.4173 | Val Loss: 261.9782 | LR: 0.000010\n",
      "Epoch 86/200: Train Loss: 762.3252 | Val Loss: 261.9335 | LR: 0.000010\n",
      "Epoch 87/200: Train Loss: 762.2343 | Val Loss: 261.8913 | LR: 0.000010\n",
      "Epoch 88/200: Train Loss: 762.1462 | Val Loss: 261.8477 | LR: 0.000010\n",
      "Epoch 89/200: Train Loss: 762.0515 | Val Loss: 261.8095 | LR: 0.000010\n",
      "Epoch 90/200: Train Loss: 761.9634 | Val Loss: 261.7662 | LR: 0.000010\n",
      "Epoch 91/200: Train Loss: 761.8723 | Val Loss: 261.7255 | LR: 0.000010\n",
      "Epoch 92/200: Train Loss: 761.7854 | Val Loss: 261.6825 | LR: 0.000010\n",
      "Epoch 93/200: Train Loss: 761.6977 | Val Loss: 261.6430 | LR: 0.000010\n",
      "Epoch 94/200: Train Loss: 761.6108 | Val Loss: 261.5987 | LR: 0.000010\n",
      "Epoch 95/200: Train Loss: 761.5227 | Val Loss: 261.5594 | LR: 0.000010\n",
      "Epoch 96/200: Train Loss: 761.4394 | Val Loss: 261.5167 | LR: 0.000010\n",
      "Epoch 97/200: Train Loss: 761.3529 | Val Loss: 261.4771 | LR: 0.000010\n",
      "Epoch 98/200: Train Loss: 761.2704 | Val Loss: 261.4333 | LR: 0.000010\n",
      "Epoch 99/200: Train Loss: 761.1868 | Val Loss: 261.3931 | LR: 0.000010\n",
      "Epoch 100/200: Train Loss: 761.1065 | Val Loss: 261.3510 | LR: 0.000010\n",
      "Epoch 101/200: Train Loss: 761.0256 | Val Loss: 261.3120 | LR: 0.000010\n",
      "Epoch 102/200: Train Loss: 760.9467 | Val Loss: 261.2690 | LR: 0.000010\n",
      "Epoch 103/200: Train Loss: 760.8665 | Val Loss: 261.2299 | LR: 0.000010\n",
      "Epoch 104/200: Train Loss: 760.7921 | Val Loss: 261.1885 | LR: 0.000010\n",
      "Epoch 105/200: Train Loss: 760.7119 | Val Loss: 261.1504 | LR: 0.000010\n",
      "Epoch 106/200: Train Loss: 760.6363 | Val Loss: 261.1079 | LR: 0.000010\n",
      "Epoch 107/200: Train Loss: 760.5566 | Val Loss: 261.0697 | LR: 0.000010\n",
      "Epoch 108/200: Train Loss: 760.4830 | Val Loss: 261.0286 | LR: 0.000010\n",
      "Epoch 109/200: Train Loss: 760.4048 | Val Loss: 260.9924 | LR: 0.000010\n",
      "Epoch 110/200: Train Loss: 760.3275 | Val Loss: 260.9495 | LR: 0.000010\n",
      "Epoch 111/200: Train Loss: 760.2510 | Val Loss: 260.9096 | LR: 0.000010\n",
      "Epoch 112/200: Train Loss: 760.1744 | Val Loss: 260.8675 | LR: 0.000010\n",
      "Epoch 113/200: Train Loss: 760.0980 | Val Loss: 260.8305 | LR: 0.000010\n",
      "Epoch 114/200: Train Loss: 760.0253 | Val Loss: 260.7865 | LR: 0.000010\n",
      "Epoch 115/200: Train Loss: 759.9473 | Val Loss: 260.7498 | LR: 0.000010\n",
      "Epoch 116/200: Train Loss: 759.8698 | Val Loss: 260.7085 | LR: 0.000010\n",
      "Epoch 117/200: Train Loss: 759.7966 | Val Loss: 260.6722 | LR: 0.000010\n",
      "Epoch 118/200: Train Loss: 759.7206 | Val Loss: 260.6324 | LR: 0.000010\n",
      "Epoch 119/200: Train Loss: 759.6455 | Val Loss: 260.5965 | LR: 0.000010\n",
      "Epoch 120/200: Train Loss: 759.5703 | Val Loss: 260.5564 | LR: 0.000010\n",
      "Epoch 121/200: Train Loss: 759.4948 | Val Loss: 260.5195 | LR: 0.000010\n",
      "Epoch 122/200: Train Loss: 759.4192 | Val Loss: 260.4808 | LR: 0.000010\n",
      "Epoch 123/200: Train Loss: 759.3447 | Val Loss: 260.4480 | LR: 0.000010\n",
      "Epoch 124/200: Train Loss: 759.2695 | Val Loss: 260.4102 | LR: 0.000010\n",
      "Epoch 125/200: Train Loss: 759.1953 | Val Loss: 260.3667 | LR: 0.000010\n",
      "Epoch 126/200: Train Loss: 759.1190 | Val Loss: 260.3337 | LR: 0.000010\n",
      "Epoch 127/200: Train Loss: 759.0447 | Val Loss: 260.2959 | LR: 0.000010\n",
      "Epoch 128/200: Train Loss: 758.9695 | Val Loss: 260.2554 | LR: 0.000010\n",
      "Epoch 129/200: Train Loss: 758.8938 | Val Loss: 260.2212 | LR: 0.000010\n",
      "Epoch 130/200: Train Loss: 758.8205 | Val Loss: 260.1850 | LR: 0.000010\n",
      "Epoch 131/200: Train Loss: 758.7484 | Val Loss: 260.1444 | LR: 0.000010\n",
      "Epoch 132/200: Train Loss: 758.6762 | Val Loss: 260.1116 | LR: 0.000010\n",
      "Epoch 133/200: Train Loss: 758.6038 | Val Loss: 260.0743 | LR: 0.000010\n",
      "Epoch 134/200: Train Loss: 758.5334 | Val Loss: 260.0395 | LR: 0.000010\n",
      "Epoch 135/200: Train Loss: 758.4620 | Val Loss: 260.0004 | LR: 0.000010\n",
      "Epoch 136/200: Train Loss: 758.3908 | Val Loss: 259.9617 | LR: 0.000010\n",
      "Epoch 137/200: Train Loss: 758.3152 | Val Loss: 259.9304 | LR: 0.000010\n",
      "Epoch 138/200: Train Loss: 758.2466 | Val Loss: 259.8921 | LR: 0.000010\n",
      "Epoch 139/200: Train Loss: 758.1722 | Val Loss: 259.8572 | LR: 0.000010\n",
      "Epoch 140/200: Train Loss: 758.1040 | Val Loss: 259.8194 | LR: 0.000010\n",
      "Epoch 141/200: Train Loss: 758.0297 | Val Loss: 259.7833 | LR: 0.000010\n",
      "Epoch 142/200: Train Loss: 757.9611 | Val Loss: 259.7481 | LR: 0.000010\n",
      "Epoch 143/200: Train Loss: 757.8904 | Val Loss: 259.7099 | LR: 0.000010\n",
      "Epoch 144/200: Train Loss: 757.8223 | Val Loss: 259.6784 | LR: 0.000010\n",
      "Epoch 145/200: Train Loss: 757.7552 | Val Loss: 259.6411 | LR: 0.000010\n",
      "Epoch 146/200: Train Loss: 757.6864 | Val Loss: 259.6098 | LR: 0.000010\n",
      "Epoch 147/200: Train Loss: 757.6230 | Val Loss: 259.5726 | LR: 0.000010\n",
      "Epoch 148/200: Train Loss: 757.5551 | Val Loss: 259.5397 | LR: 0.000010\n",
      "Epoch 149/200: Train Loss: 757.4900 | Val Loss: 259.5041 | LR: 0.000010\n",
      "Epoch 150/200: Train Loss: 757.4221 | Val Loss: 259.4708 | LR: 0.000010\n",
      "Epoch 151/200: Train Loss: 757.3564 | Val Loss: 259.4342 | LR: 0.000010\n",
      "Epoch 152/200: Train Loss: 757.2905 | Val Loss: 259.4011 | LR: 0.000010\n",
      "Epoch 153/200: Train Loss: 757.2254 | Val Loss: 259.3647 | LR: 0.000010\n",
      "Epoch 154/200: Train Loss: 757.1620 | Val Loss: 259.3339 | LR: 0.000010\n",
      "Epoch 155/200: Train Loss: 757.0974 | Val Loss: 259.2974 | LR: 0.000010\n",
      "Epoch 156/200: Train Loss: 757.0337 | Val Loss: 259.2664 | LR: 0.000010\n",
      "Epoch 157/200: Train Loss: 756.9701 | Val Loss: 259.2331 | LR: 0.000010\n",
      "Epoch 158/200: Train Loss: 756.9075 | Val Loss: 259.2018 | LR: 0.000010\n",
      "Epoch 159/200: Train Loss: 756.8411 | Val Loss: 259.1697 | LR: 0.000010\n",
      "Epoch 160/200: Train Loss: 756.7811 | Val Loss: 259.1325 | LR: 0.000010\n",
      "Epoch 161/200: Train Loss: 756.7163 | Val Loss: 259.1076 | LR: 0.000010\n",
      "Epoch 162/200: Train Loss: 756.6530 | Val Loss: 259.0689 | LR: 0.000010\n",
      "Epoch 163/200: Train Loss: 756.5888 | Val Loss: 259.0408 | LR: 0.000010\n",
      "Epoch 164/200: Train Loss: 756.5247 | Val Loss: 259.0048 | LR: 0.000010\n",
      "Epoch 165/200: Train Loss: 756.4638 | Val Loss: 258.9752 | LR: 0.000010\n",
      "Epoch 166/200: Train Loss: 756.3993 | Val Loss: 258.9454 | LR: 0.000010\n",
      "Epoch 167/200: Train Loss: 756.3392 | Val Loss: 258.9144 | LR: 0.000010\n",
      "Epoch 168/200: Train Loss: 756.2746 | Val Loss: 258.8850 | LR: 0.000010\n",
      "Epoch 169/200: Train Loss: 756.2146 | Val Loss: 258.8533 | LR: 0.000010\n",
      "Epoch 170/200: Train Loss: 756.1528 | Val Loss: 258.8249 | LR: 0.000010\n",
      "Epoch 171/200: Train Loss: 756.0934 | Val Loss: 258.7933 | LR: 0.000010\n",
      "Epoch 172/200: Train Loss: 756.0323 | Val Loss: 258.7636 | LR: 0.000010\n",
      "Epoch 173/200: Train Loss: 755.9700 | Val Loss: 258.7323 | LR: 0.000010\n",
      "Epoch 174/200: Train Loss: 755.9122 | Val Loss: 258.7025 | LR: 0.000010\n",
      "Epoch 175/200: Train Loss: 755.8505 | Val Loss: 258.6731 | LR: 0.000010\n",
      "Epoch 176/200: Train Loss: 755.7932 | Val Loss: 258.6416 | LR: 0.000010\n",
      "Epoch 177/200: Train Loss: 755.7335 | Val Loss: 258.6135 | LR: 0.000010\n",
      "Epoch 178/200: Train Loss: 755.6764 | Val Loss: 258.5858 | LR: 0.000010\n",
      "Epoch 179/200: Train Loss: 755.6200 | Val Loss: 258.5556 | LR: 0.000010\n",
      "Epoch 180/200: Train Loss: 755.5614 | Val Loss: 258.5282 | LR: 0.000010\n",
      "Epoch 181/200: Train Loss: 755.5066 | Val Loss: 258.4984 | LR: 0.000010\n",
      "Epoch 182/200: Train Loss: 755.4495 | Val Loss: 258.4701 | LR: 0.000010\n",
      "Epoch 183/200: Train Loss: 755.3930 | Val Loss: 258.4424 | LR: 0.000010\n",
      "Epoch 184/200: Train Loss: 755.3383 | Val Loss: 258.4108 | LR: 0.000010\n",
      "Epoch 185/200: Train Loss: 755.2809 | Val Loss: 258.3856 | LR: 0.000010\n",
      "Epoch 186/200: Train Loss: 755.2228 | Val Loss: 258.3571 | LR: 0.000010\n",
      "Epoch 187/200: Train Loss: 755.1696 | Val Loss: 258.3298 | LR: 0.000010\n",
      "Epoch 188/200: Train Loss: 755.1119 | Val Loss: 258.3059 | LR: 0.000010\n",
      "Epoch 189/200: Train Loss: 755.0570 | Val Loss: 258.2717 | LR: 0.000010\n",
      "Epoch 190/200: Train Loss: 755.0017 | Val Loss: 258.2479 | LR: 0.000010\n",
      "Epoch 191/200: Train Loss: 754.9439 | Val Loss: 258.2206 | LR: 0.000010\n",
      "Epoch 192/200: Train Loss: 754.8913 | Val Loss: 258.1909 | LR: 0.000010\n",
      "Epoch 193/200: Train Loss: 754.8333 | Val Loss: 258.1631 | LR: 0.000010\n",
      "Epoch 194/200: Train Loss: 754.7793 | Val Loss: 258.1361 | LR: 0.000010\n",
      "Epoch 195/200: Train Loss: 754.7225 | Val Loss: 258.1064 | LR: 0.000010\n",
      "Epoch 196/200: Train Loss: 754.6695 | Val Loss: 258.0799 | LR: 0.000010\n",
      "Epoch 197/200: Train Loss: 754.6153 | Val Loss: 258.0516 | LR: 0.000010\n",
      "Epoch 198/200: Train Loss: 754.5610 | Val Loss: 258.0258 | LR: 0.000010\n",
      "Epoch 199/200: Train Loss: 754.5073 | Val Loss: 257.9960 | LR: 0.000010\n",
      "Epoch 200/200: Train Loss: 754.4539 | Val Loss: 257.9730 | LR: 0.000010\n",
      "Best model restored from epoch 200\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10 #число фичей\n",
    "m = 4 #число таргетов\n",
    "\n",
    "X, y = make_regression(n_samples=30000, n_features=n, n_targets=m, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "# scaler_X = StandardScaler()\n",
    "# X_train = scaler_X.fit_transform(X_train)\n",
    "# X_val = scaler_X.transform(X_val)\n",
    "# scaler_y = StandardScaler()\n",
    "# y_train = scaler_y.fit_transform(y_train)\n",
    "# y_val = scaler_y.transform(y_val)\n",
    "\n",
    "model_small = Sequential()\n",
    "model_small.add(Linear(n, 64))\n",
    "model_small.add(ReLU())\n",
    "model_small.add(Linear(64, 32))\n",
    "model_small.add(ELU())\n",
    "model_small.add(Linear(32, m))\n",
    "\n",
    "model_med = Sequential()\n",
    "model_med.add(Linear(n, 64))\n",
    "model_med.add(BatchNormalization())\n",
    "model_med.add(ReLU())\n",
    "model_med.add(Dropout(0.5))\n",
    "model_med.add(Linear(64, 32))\n",
    "model_med.add(BatchNormalization())\n",
    "model_med.add(ELU())\n",
    "model_med.add(Linear(32, m))\n",
    "\n",
    "model_large = Sequential()\n",
    "model_large.add(Linear(n, 64))\n",
    "model_large.add(BatchNormalization())\n",
    "model_large.add(ReLU())\n",
    "model_large.add(Dropout(0.5))\n",
    "model_large.add(Linear(64, 64))\n",
    "model_large.add(BatchNormalization())\n",
    "model_large.add(ELU())\n",
    "model_large.add(Dropout(0.5))\n",
    "model_large.add(Linear(64, 32))\n",
    "model_large.add(BatchNormalization())\n",
    "model_large.add(Gelu())\n",
    "model_large.add(Dropout(0.5))\n",
    "model_large.add(Linear(32, m))\n",
    "\n",
    "model = model_small\n",
    "optimizer = AdamOptimizer(model, lr=0.01)\n",
    "criterion = MSECriterion()\n",
    "scheduler = ReduceLROnPlateau(optimizer, initial_lr=0.01, factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "trainer = MSERegressionTrainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    patience=10\n",
    ")\n",
    "train_history, val_history = trainer.train(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    epochs=200,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# def evaluate_model(X, y, model):\n",
    "#     model.evaluate()\n",
    "#     predictions = model.forward(X)\n",
    "#     mse = np.mean((predictions - y)**2)\n",
    "#     print(f\"Test MSE: {mse:.4f}\")\n",
    "#     return predictions\n",
    "# test_predictions = evaluate_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
