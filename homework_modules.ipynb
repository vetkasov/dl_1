{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3MeKai5Xj6eX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwlFrG-Tj6eY"
   },
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W8BLmtZ3j6eZ"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box)\n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`:\n",
    "\n",
    "        output = module.forward(input)\n",
    "\n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule.\n",
    "\n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self,input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "\n",
    "        This includes\n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "\n",
    "        Make sure to both store the data in `output` field and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        # The easiest case:\n",
    "\n",
    "        # self.output = input\n",
    "        # return self.output\n",
    "\n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input.\n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "\n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "\n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        # The easiest case:\n",
    "\n",
    "        # self.gradInput = gradOutput\n",
    "        # return self.gradInput\n",
    "\n",
    "        pass\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKRkIjT8j6eZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb98PPpJj6ea"
   },
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7y2lav4dj6ea"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially.\n",
    "\n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `output`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "\n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "\n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})\n",
    "\n",
    "\n",
    "        Just write a little loop.\n",
    "        \"\"\"\n",
    "        # for i in range():\n",
    "        #     f'y_{i}' = self.modules[i].forward(f'y_{i-1}')\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = input\n",
    "        for module in self.modules:\n",
    "            self.output = module.updateOutput(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "\n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)\n",
    "            gradInput = module[0].backward(input, g_1)\n",
    "\n",
    "\n",
    "        !!!\n",
    "\n",
    "        To ech module you need to provide the input, module saw while forward pass,\n",
    "        it is used while computing gradients.\n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
    "        and NOT `input` to this Sequential module.\n",
    "\n",
    "        !!!\n",
    "\n",
    "        \"\"\"\n",
    "        # Your code goes here. ################################################\n",
    "        outputs = [input]\n",
    "        for module in self.modules:\n",
    "            outputs.append(module.updateOutput(outputs[-1]))\n",
    "        current_grad = gradOutput\n",
    "        for i in range(len(self.modules)-1, -1, -1):\n",
    "            current_grad = self.modules[i].backward(outputs[i], current_grad)\n",
    "    \n",
    "        self.gradInput = current_grad\n",
    "        return self.gradInput\n",
    "\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "\n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "\n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "\n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfXdYfO4j6ea"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuwvBkuNj6ea",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1 (0.2). Linear transform layer\n",
    "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D0uoyqkpj6ea"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation\n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
    "\n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.W.T) + self.b\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.dot(gradOutput, self.W)\n",
    "        return self.gradInput\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW += np.dot(gradOutput.T, input)\n",
    "        self.gradb += np.sum(gradOutput, axis=0)\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "\n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNOnHXZJj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. (0.2) SoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
    "\n",
    "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VIValI0hj6eb"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        e = np.exp(self.output)\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = e/np.sum(e, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        dot_product = np.sum(self.output * gradOutput, axis=1, keepdims=True)\n",
    "        self.gradInput = self.output * (gradOutput - dot_product)\n",
    "        return self.gradInput\n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy3DJjynj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. (0.2) LogSoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
    "\n",
    "The main goal of this layer is to be used in computation of log-likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xo7DRdAJj6eb"
   },
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(LogSoftMax, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = self.output - np.log(np.sum(np.exp(self.output), axis=1, keepdims=True))\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        softmax = np.exp(self.output)\n",
    "        \n",
    "        # Compute sum over gradients (batch-wise)\n",
    "        sum_grad = np.sum(gradOutput, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute gradient\n",
    "        self.gradInput = gradOutput - softmax * sum_grad\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LogSoftMax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP5QdmmPj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. (0.3) Batch normalization\n",
    "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
    "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
    "```\n",
    "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
    "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
    "```\n",
    "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
    "\n",
    "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fGTTDqVgj6eb"
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(Module):\n",
    "    EPS = 1e-3\n",
    "    def __init__(self, alpha = 0.):\n",
    "        super(BatchNormalization, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.moving_mean = None\n",
    "        self.moving_variance = None\n",
    "        self.cache = None\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            batch_mean = np.mean(input, axis=0, keepdims=True)\n",
    "            batch_variance = np.var(input, axis=0, keepdims=True)\n",
    "            self.moving_mean = self.moving_mean*self.alpha + batch_mean*(1 - self.alpha) if self.moving_mean is not None else batch_mean\n",
    "            self.moving_variance = self.moving_variance*self.alpha + batch_variance*(1 - self.alpha) if self.moving_variance is not None else batch_variance\n",
    "            self.output = (input - batch_mean) / np.sqrt(batch_variance+ self.EPS)\n",
    "            self.cache = (batch_mean, batch_variance)\n",
    "        else:\n",
    "            self.output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS) \n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            batch_mean, batch_variance = self.cache\n",
    "            m = input.shape[0]\n",
    "            d_var = np.sum(gradOutput * (input - batch_mean) * -0.5 * (batch_variance + self.EPS)**(-1.5), axis=0, keepdims=True)\n",
    "            d_mean = np.sum(gradOutput * -1 / np.sqrt(batch_variance + self.EPS), axis=0, keepdims=True) + \\\n",
    "                   d_var * np.sum(-2 * (input - batch_mean), axis=0, keepdims=True) / m\n",
    "            # d_mean = np.sum(gradOutput * (-1) / np.sqrt(batch_variance + self.EPS), axis=0) + \\\n",
    "            #        d_var * (-2) * np.mean(input - batch_mean, axis=0)\n",
    "            self.gradInput = gradOutput / np.sqrt(batch_variance + self.EPS) + \\\n",
    "                            d_var * 2 * (input - batch_mean) / m + \\\n",
    "                            d_mean / m\n",
    "        else:\n",
    "            self.gradInput = gradOutput / np.sqrt(self.moving_variance + self.EPS)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"BatchNormalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8XUS3Lt-j6eb"
   },
   "outputs": [],
   "source": [
    "class ChannelwiseScaling(Module):\n",
    "    \"\"\"\n",
    "       Implements linear transform of input y = gamma * x + beta\n",
    "       where gamma, beta - learnable vectors of length x.shape[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_out):\n",
    "        super(ChannelwiseScaling, self).__init__()\n",
    "\n",
    "        stdv = 1./np.sqrt(n_out)\n",
    "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "\n",
    "        self.gradGamma = np.zeros_like(self.gamma)\n",
    "        self.gradBeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = input * self.gamma + self.beta\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * self.gamma\n",
    "        return self.gradInput\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
    "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        self.gradGamma.fill(0)\n",
    "        self.gradBeta.fill(0)\n",
    "\n",
    "    def getParameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        return [self.gradGamma, self.gradBeta]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ChannelwiseScaling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA5zjM3jj6eb"
   },
   "source": [
    "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gackeo1cj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. (0.3) Dropout\n",
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NmLQV3jXj6eb"
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            self.mask = np.random.binomial(size=input.shape, n=1, p=(1 - self.p)).astype('float') / (1 - self.p)\n",
    "            self.output = input * self.mask\n",
    "        else:\n",
    "            self.output = input\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        if self.training:\n",
    "            self.gradInput = gradOutput * self.mask\n",
    "        else:\n",
    "            self.gradInput = gradOutput\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WHGIqJFlhz2"
   },
   "source": [
    "## 6. (2.0) Conv2d\n",
    "Implement [**Conv2d**](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). Use only this list of parameters: (in_channels, out_channels, kernel_size, stride, padding, bias, padding_mode) and fix dilation=1 and groups=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c1RjNoEXlOHP"
   },
   "outputs": [],
   "source": [
    "# class Conv2d(Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size,\n",
    "#                  stride=1, padding=0, bias=True, padding_mode='zeros'):\n",
    "#         super(Conv2d, self).__init__()\n",
    "\n",
    "#         self.in_channels = in_channels\n",
    "#         self.out_channels = out_channels\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "#         self.bias = bias\n",
    "#         self.padding_mode = padding_mode\n",
    "\n",
    "#     def updateOutput(self, input):\n",
    "#         # Your code goes here. ################################################\n",
    "        \n",
    "#         return  self.output\n",
    "\n",
    "#     def updateGradInput(self, input, gradOutput):\n",
    "#         # Your code goes here. ################################################\n",
    "        \n",
    "#         return self.gradInput\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return \"Conv2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='valid', bias=True, padding_mode='zeros'):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride, stride)\n",
    "        self.padding = padding\n",
    "        self.padding_mode = padding_mode\n",
    "        self.bias = bias\n",
    "\n",
    "        # Инициализация параметров\n",
    "        self.weight = np.random.randn(out_channels, in_channels, *self.kernel_size) * 0.1\n",
    "        if self.bias:\n",
    "            self.bias_params = np.zeros(out_channels)\n",
    "        \n",
    "        # Градиенты\n",
    "        self.gradWeight = np.zeros_like(self.weight)\n",
    "        if self.bias:\n",
    "            self.gradBias = np.zeros_like(self.bias_params)\n",
    "        \n",
    "        self.last_input = None\n",
    "        self.last_padding = ((0, 0), (0, 0))\n",
    "\n",
    "    def _calculate_same_padding(self, in_height, in_width):\n",
    "        out_height = np.ceil(in_height / self.stride[0]).astype(int)\n",
    "        out_width = np.ceil(in_width / self.stride[1]).astype(int)\n",
    "        pad_h = max(0, (out_height - 1) * self.stride[0] + self.kernel_size[0] - in_height)\n",
    "        pad_w = max(0, (out_width - 1) * self.stride[1] + self.kernel_size[1] - in_width)\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "        return ((pad_top, pad_bottom), (pad_left, pad_right))\n",
    "\n",
    "    def _apply_padding(self, input, padding):\n",
    "        if padding == ((0, 0), (0, 0)):\n",
    "            return input\n",
    "        \n",
    "        pad_top, pad_bottom = padding[0]\n",
    "        pad_left, pad_right = padding[1]\n",
    "        \n",
    "        pad_width = (\n",
    "            (0, 0), \n",
    "            (0, 0),\n",
    "            (pad_top, pad_bottom),\n",
    "            (pad_left, pad_right)\n",
    "        )\n",
    "        \n",
    "        if self.padding_mode == 'zeros':\n",
    "            return np.pad(input, pad_width, mode='constant', constant_values=0)\n",
    "        elif self.padding_mode == 'replicate':\n",
    "            return np.pad(input, pad_width, mode='edge')\n",
    "        elif self.padding_mode == 'reflect':\n",
    "            return np.pad(input, pad_width, mode='symmetric')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported padding mode: {self.padding_mode}\")\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        batch_size, in_channels, in_height, in_width = input.shape\n",
    "        \n",
    "        if self.padding == 'same':\n",
    "            current_padding = self._calculate_same_padding(in_height, in_width)\n",
    "        elif isinstance(self.padding, int):\n",
    "            current_padding = ((self.padding, self.padding), (self.padding, self.padding))\n",
    "        elif isinstance(self.padding, tuple) and len(self.padding) == 2:\n",
    "            if isinstance(self.padding[0], int):\n",
    "                current_padding = ((self.padding[0], self.padding[0]), (self.padding[1], self.padding[1]))\n",
    "            else:\n",
    "                current_padding = self.padding\n",
    "        else:\n",
    "            current_padding = ((0, 0), (0, 0))\n",
    "        \n",
    "        input_padded = self._apply_padding(input, current_padding)\n",
    "        self.last_input = input_padded\n",
    "        self.last_padding = current_padding\n",
    "        out_height = (input_padded.shape[2] - self.kernel_size[0]) // self.stride[0] + 1\n",
    "        out_width = (input_padded.shape[3] - self.kernel_size[1]) // self.stride[1] + 1\n",
    "        self.output = np.zeros((batch_size, self.out_channels, out_height, out_width))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for oh in range(out_height):\n",
    "                    for ow in range(out_width):\n",
    "                        h_start = oh * self.stride[0]\n",
    "                        w_start = ow * self.stride[1]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "                        \n",
    "                        window = input_padded[b, :, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, oc, oh, ow] = np.sum(window * self.weight[oc])\n",
    "                        \n",
    "                        if self.bias:\n",
    "                            self.output[b, oc, oh, ow] += self.bias_params[oc]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        batch_size, _, out_h, out_w = gradOutput.shape\n",
    "        input_padded = self.last_input\n",
    "        grad_input_padded = np.zeros_like(input_padded)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for oh in range(out_h):\n",
    "                    for ow in range(out_w):\n",
    "                        h_start = oh * self.stride[0]\n",
    "                        w_start = ow * self.stride[1]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "                        \n",
    "                        window = input_padded[b, :, h_start:h_end, w_start:w_end]\n",
    "                        self.gradWeight[oc] += window * gradOutput[b, oc, oh, ow]\n",
    "                        grad_input_padded[b, :, h_start:h_end, w_start:w_end] += self.weight[oc] * gradOutput[b, oc, oh, ow]\n",
    "                \n",
    "                if self.bias:\n",
    "                    self.gradBias[oc] += np.sum(gradOutput[b, oc])\n",
    "        \n",
    "        pad_top, pad_bottom = self.last_padding[0]\n",
    "        pad_left, pad_right = self.last_padding[1]\n",
    "        self.gradInput = grad_input_padded[:, :, pad_top:-pad_bottom if pad_bottom !=0 else None, pad_left:-pad_right if pad_right !=0 else None]\n",
    "        \n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Conv2d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = [\n",
    "#             {'batch_size': 8, 'in_channels': 3, 'out_channels': 6, 'height': 32, 'width': 32,\n",
    "#              'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True, 'padding_mode': 'zeros'}\n",
    "#         ]\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# for _ in range(100):\n",
    "#     for params in hyperparams:\n",
    "#         batch_size = params['batch_size']\n",
    "#         in_channels = params['in_channels']\n",
    "#         out_channels = params['out_channels']\n",
    "#         height = params['height']\n",
    "#         width = params['width']\n",
    "#         kernel_size = params['kernel_size']\n",
    "#         stride = params['stride']\n",
    "#         padding = params['padding']\n",
    "#         bias = params['bias']\n",
    "#         padding_mode = params['padding_mode']\n",
    "\n",
    "#         custom_layer = Conv2d(in_channels, out_channels, kernel_size,\n",
    "#                             stride=stride, padding=padding, bias=bias,\n",
    "#                             padding_mode=padding_mode)\n",
    "#         custom_layer.train()\n",
    "\n",
    "#         torch_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "#                                     stride=stride, padding=padding, bias=bias,\n",
    "#                                     padding_mode=padding_mode)\n",
    "\n",
    "#         custom_layer.weight = torch_layer.weight.detach().numpy().copy()\n",
    "# #         if bias:\n",
    "# #             custom_layer.bias = torch_layer.bias.detach().numpy().copy()\n",
    "\n",
    "#         layer_input = np.random.randn(batch_size, in_channels, height, width).astype(np.float32)\n",
    "#         input_var = torch.tensor(layer_input, requires_grad=True)\n",
    "\n",
    "#         custom_output = custom_layer.updateOutput(layer_input)\n",
    "#         torch_output = torch_layer(input_var)\n",
    "#         self.assertTrue(\n",
    "#             np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n",
    "\n",
    "#         next_layer_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n",
    "#         custom_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "#         torch_output.backward(torch.tensor(next_layer_grad))\n",
    "#         torch_grad = input_var.grad.detach().numpy()\n",
    "#         self.assertTrue(\n",
    "#         np.allclose(torch_grad, custom_grad, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_layer.bias.detach().numpy().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "updUVZE9qixP",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. (0.5) Implement [**MaxPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [**AvgPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html). Use only parameters like kernel_size, stride, padding (negative infinity for maxpool and zero for avgpool) and other parameters fixed as in framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Qys58EzkqhLj"
   },
   "outputs": [],
   "source": [
    "class MaxPool2d(Module):\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super(MaxPool2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
    "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
    "        self.cache = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        batch, num_channels, H_in, W_in = input.shape\n",
    "        H_out = ((H_in + 2*self.padding[0] - self.kernel_size[0])//self.stride[0] + 1)\n",
    "        W_out = ((W_in + 2*self.padding[1] - self.kernel_size[1])//self.stride[1] + 1)\n",
    "        \n",
    "        self.output = np.zeros((batch, num_channels, H_out, W_out))\n",
    "        self.indices = np.zeros((batch, num_channels, H_out, W_out, 2), dtype=int)\n",
    "        padding = np.pad(input, \n",
    "                            ((0,0), (0,0), (self.padding[0],self.padding[0]), (self.padding[1], self.padding[1])), \n",
    "                            mode='constant')\n",
    "        for b in range(batch):\n",
    "            for c in range(num_channels):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start = i * self.stride[0]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_start = j * self.stride[1]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "\n",
    "                        window = padding[b, c, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, c, i, j] = np.max(window)\n",
    "                        max_idx = np.unravel_index(np.argmax(window), window.shape)\n",
    "                        self.indices[b, c, i, j] = [h_start + max_idx[0], w_start + max_idx[1]]\n",
    "        self.cache = (batch, num_channels, H_in, W_in)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        batch, num_channels, H_in, W_in = self.cache\n",
    "        out_shape = (batch, num_channels, \n",
    "                       H_in + 2*self.padding[0], W_in + 2*self.padding[1])\n",
    "        self.gradInput = np.zeros(out_shape)\n",
    "        for b in range(batch):\n",
    "            for c in range(num_channels):\n",
    "                for i in range(gradOutput.shape[2]):\n",
    "                    for j in range(gradOutput.shape[3]):\n",
    "                        h, w = self.indices[b, c, i, j]\n",
    "                        self.gradInput[b, c, h, w] += gradOutput[b, c, i, j]\n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            self.gradInput = self.gradInput[:, :, self.padding[0]:-self.padding[0], self.padding[1]:-self.padding[1]] if self.padding[0] > 0 and self.padding[1] > 0 else \\\n",
    "                           self.gradInput[:, :, self.padding[0]:-self.padding[0], :] if self.padding[0] > 0 else \\\n",
    "                           self.gradInput[:, :, :, self.padding[1]:-self.padding[1]]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MaxPool2d\"\n",
    "\n",
    "class AvgPool2d(Module):\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super(AvgPool2d, self).__init__()\n",
    "\n",
    "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
    "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
    "        self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
    "        self.cache = None\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        batch, num_channels, H_in, W_in = input.shape\n",
    "        H_out = ((H_in + 2*self.padding[0] - self.kernel_size[0])//self.stride[0] + 1)\n",
    "        W_out = ((W_in + 2*self.padding[1] - self.kernel_size[1])//self.stride[1] + 1)\n",
    "        \n",
    "        self.output = np.zeros((batch, num_channels, H_out, W_out))\n",
    "        self.indices = np.zeros((batch, num_channels, H_out, W_out, 2), dtype=int)\n",
    "        padding = np.pad(input, \n",
    "                            ((0,0), (0,0), (self.padding[0],self.padding[0]), (self.padding[1], self.padding[1])), \n",
    "                            mode='constant')\n",
    "        for b in range(batch):\n",
    "            for c in range(num_channels):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start = i * self.stride[0]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_start = j * self.stride[1]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "\n",
    "                        window = padding[b, c, h_start:h_end, w_start:w_end]\n",
    "                        self.output[b, c, i, j] = np.mean(window)\n",
    "        self.cache = (batch, num_channels, H_in, W_in)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        batch, num_channels, H_in, W_in = self.cache\n",
    "        out_shape = (batch, num_channels, \n",
    "                       H_in + 2*self.padding[0], W_in + 2*self.padding[1])\n",
    "        self.gradInput = np.zeros(out_shape)\n",
    "        norm_win = 1 / (self.kernel_size[0] * self.kernel_size[1])\n",
    "        for b in range(batch):\n",
    "            for c in range(num_channels):\n",
    "                for i in range(gradOutput.shape[2]):\n",
    "                    for j in range(gradOutput.shape[3]):\n",
    "                        h_start = i * self.stride[0]\n",
    "                        h_end = h_start + self.kernel_size[0]\n",
    "                        w_start = j * self.stride[1]\n",
    "                        w_end = w_start + self.kernel_size[1]\n",
    "                        self.gradInput[b, c, h_start:h_end, w_start:w_end] += gradOutput[b, c, i, j] * norm_win\n",
    "                        \n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            self.gradInput = self.gradInput[:, :, self.padding[0]:-self.padding[0], self.padding[1]:-self.padding[1]] if self.padding[0] > 0 and self.padding[1] > 0 else \\\n",
    "                           self.gradInput[:, :, self.padding[0]:-self.padding[0], :] if self.padding[0] > 0 else \\\n",
    "                           self.gradInput[:, :, :, self.padding[1]:-self.padding[1]]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"AvgPool2d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTN5R3CwrukV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. (0.3) Implement **GlobalMaxPool2d** and **GlobalAvgPool2d**. They do not have testing and parameters are up to you but they must aggregate information within channels. Write test functions for these layers on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYeBQDBhtViy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9. (0.2) Implement [**Flatten**](https://pytorch.org/docs/stable/generated/torch.flatten.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Module):\n",
    "    def __init__(self, start_dim=0, end_dim=-1):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.start_dim = self.start_dim if self.start_dim >= 0 else len(input.shape) + self.start_dim\n",
    "        self.end_dim = self.end_dim if self.end_dim >= 0 else len(input.shape) + self.end_dim\n",
    "        to_shape = list(input.shape[:self.start_dim])\n",
    "        flattened_size = 1\n",
    "        for _ in input.shape[self.start_dim:self.end_dim+1]:\n",
    "            flattened_size *= _\n",
    "        to_shape.append(flattened_size)\n",
    "        to_shape.extend(input.shape[self.end_dim+1:])\n",
    "        self.output = input.reshape(to_shape)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = gradOutput.reshape(input.shape)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Flatten\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o36vPHSSj6eb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_pryRQIj6ec"
   },
   "source": [
    "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sgm8bXjKj6ec"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB0UHGagj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10. (0.1) Leaky ReLU\n",
    "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "agwfkwO0j6ec"
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "\n",
    "        self.slope = slope\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.where(input > 0, input, input * self.slope)\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = np.where(input > 0, gradOutput, gradOutput * self.slope)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-STyecvj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 11. (0.1) ELU\n",
    "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jJSzEu1mj6ec"
   },
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        super(ELU, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.where(input > 0, input, self.alpha*(np.exp(input) - 1))\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = np.where(input > 0, gradOutput, self.alpha*(np.exp(input))*gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ELU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn3C7KTqj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 12. (0.1) SoftPlus\n",
    "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xcDPMssrj6ec"
   },
   "outputs": [],
   "source": [
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.log(1 + np.exp(input))\n",
    "        return  self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = gradOutput/(1 + np.exp(-input))\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw3PeZjOuo0e",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 13. (0.2) Gelu\n",
    "Implement [**Gelu**](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SdieE0Dtuo8j"
   },
   "outputs": [],
   "source": [
    "class Gelu(Module):\n",
    "    def __init__(self):\n",
    "        super(Gelu, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        # self.output = 0.5*input*(1 + np.tanh(np.sqrt(2/np.pi) * (input + 0.044715*input**3))) #НЕ РАБОТАЕТ, ПОСКОЛЬКУ В pytorch ИСПОЛЬЗУЕТСЯ ДРУГАЯ АППРОКСИМАЦИЯ -> на тесте не сходится\n",
    "        self.output = 0.5 * input * (1 + torch.erf(torch.from_numpy(input / np.sqrt(2))).numpy())\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = gradOutput * (0.5*(1 + torch.erf(torch.from_numpy(input/np.sqrt(2))).numpy()) + (input * np.exp(-0.5*input**2))/np.sqrt(2*np.pi))\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Gelu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55p7UvPAj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NFaxZaqj6ec"
   },
   "source": [
    "Criterions are used to score the models answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "XGu45A8qj6ec"
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function\n",
    "            associated to the criterion and return the result.\n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result.\n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuU26xkpj6ec"
   },
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- target: **`batch_size x n_feats`**\n",
    "- output: **scalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-i3VNuHhj6ec"
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8LKLWNVj6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 14. (0.2) Negative LogLikelihood criterion (numerically unstable)\n",
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
    "- input:   **`batch_size x n_feats`** - probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "die7KvW6j6ec"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterionUnstable(Criterion):\n",
    "    EPS = 1e-15\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterionUnstable, self)\n",
    "        super(ClassNLLCriterionUnstable, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        # return loss of loss function\n",
    "\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = -np.sum(target * np.log(input_clamp)) / target.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        # return gradient of loss function\n",
    "\n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = -target / (input_clamp * target.shape[0])\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterionUnstable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHr_JbU5j6ec",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 15. (0.3) Negative LogLikelihood criterion (numerically stable)\n",
    "- input:   **`batch_size x n_feats`** - log probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n",
    "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "v7N8bVP9j6ec"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "\n",
    "    def updateOutput(self, input, target):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = -np.sum(target * input) / target.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = -target / target.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-ZnhKxaj6ed"
   },
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC2Bf1PP2Ios"
   },
   "source": [
    "1-я часть задания: реализация слоев, лосей и функций активации - 5 баллов. \\\\\n",
    "2-я часть задания: реализация моделей на своих классах. Что должно быть:\n",
    "  1. Выберите оптимизатор и реализуйте его, чтоб он работал с вами классами. - 1 балл.✅\n",
    "  2. Модель для задачи мультирегрессии на выбраных вами данных. Использовать FCNN, dropout, batchnorm, MSE. Пробуйте различные фукнции активации. Для первой модели попробуйте большую, среднюю и маленькую модель. - 1 балл.✅\n",
    "  3. Модель для задачи мультиклассификации на MNIST. Использовать свёртки, макспулы, флэттэны, софтмаксы - 1 балла.\n",
    "  4. Автоэнкодер для выбранных вами данных. Должен быть на свёртках и полносвязных слоях, дропаутах, батчнормах и тд. - 2 балла. \\\\\n",
    "\n",
    "Дополнительно в оценке каждой модели будет учитываться:\n",
    "1. Наличие правильно выбранной метрики и лосс функции.✅\n",
    "2. Отрисовка графиков лосей и метрик на трейне-валидации. Проверка качества модели на тесте.✅\n",
    "3. Наличие шедулера для lr.✅\n",
    "4. Наличие вормапа.\n",
    "5. Наличие механизма ранней остановки и сохранение лучшей модели.✅\n",
    "6. Свитч лося (метрики) и оптимайзера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, model, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        self.params = self._get_all_parameters()\n",
    "        for param in self.params:\n",
    "            self.m.append(np.zeros_like(param))\n",
    "            self.v.append(np.zeros_like(param))\n",
    "    \n",
    "    def _get_all_parameters(self):\n",
    "        params = []\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        \n",
    "        for module in modules:\n",
    "            if hasattr(module, 'getParameters'):\n",
    "                module_params = module.getParameters()\n",
    "                if module_params:  # Skip empty lists\n",
    "                    if isinstance(module_params[0], (list, tuple, np.ndarray)):\n",
    "                        params.extend(module_params)\n",
    "                    else:\n",
    "                        params.append(module_params)\n",
    "        return params\n",
    "    \n",
    "    def _get_all_gradients(self):\n",
    "        grads = []\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        \n",
    "        for module in modules:\n",
    "            if hasattr(module, 'getGradParameters'):\n",
    "                module_grads = module.getGradParameters()\n",
    "                if module_grads:  # Skip empty lists\n",
    "                    if isinstance(module_grads[0], (list, tuple, np.ndarray)):\n",
    "                        grads.extend(module_grads)\n",
    "                    else:\n",
    "                        grads.append(module_grads)\n",
    "        return grads\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        grads = self._get_all_gradients()\n",
    "        \n",
    "        for i, (param, grad) in enumerate(zip(self.params, grads)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        for module in modules:\n",
    "            if hasattr(module, 'zeroGradParameters'):\n",
    "                module.zeroGradParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.current_lr = initial_lr\n",
    "    \n",
    "    def step(self, epoch=None, metrics=None):\n",
    "        self.update_lr(epoch, metrics)\n",
    "        self.apply_lr()\n",
    "    \n",
    "    def update_lr(self, epoch, metrics):\n",
    "        pass\n",
    "    \n",
    "    def apply_lr(self):\n",
    "        self.optimizer.lr = self.current_lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.current_lr\n",
    "\n",
    "\n",
    "class StepLR(LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, step_size, gamma=0.1):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def update_lr(self, epoch, metrics=None):\n",
    "        if epoch > 0 and epoch % self.step_size == 0:\n",
    "            self.current_lr = self.initial_lr * (self.gamma ** (epoch // self.step_size))\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau(LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, factor=0.1, patience=10, min_lr=1e-6):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.best_metric = None\n",
    "        self.wait = 0\n",
    "    \n",
    "    def update_lr(self, epoch, metrics):\n",
    "        if metrics is None:\n",
    "            return\n",
    "            \n",
    "        if self.best_metric is None:\n",
    "            self.best_metric = metrics\n",
    "        elif metrics > self.best_metric:\n",
    "            self.best_metric = metrics\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                new_lr = max(self.current_lr * self.factor, self.min_lr)\n",
    "                if new_lr < self.current_lr:\n",
    "                    self.current_lr = new_lr\n",
    "                    self.wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSERegressionTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, scheduler=None, patience=5):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.patience = patience\n",
    "        self.train_history = {'loss': [], 'mse': []}\n",
    "        self.val_history = {'loss': [], 'mse': []}\n",
    "        self.best_loss = np.inf\n",
    "        self.best_epoch = 0\n",
    "        self.best_params = None\n",
    "\n",
    "    def _save_best_params(self):\n",
    "        self.best_params = []\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        \n",
    "        for module in modules:\n",
    "            if hasattr(module, 'getParameters'):\n",
    "                params = module.getParameters()\n",
    "                if params:\n",
    "                    self.best_params.append([p.copy() for p in params])\n",
    "\n",
    "    def _load_best_params(self):\n",
    "        if self.best_params is None:\n",
    "            return\n",
    "            \n",
    "        idx = 0\n",
    "        modules = self.model.modules if hasattr(self.model, 'modules') else [self.model]\n",
    "        \n",
    "        for module in modules:\n",
    "            if hasattr(module, 'getParameters'):\n",
    "                params = module.getParameters()\n",
    "                if params:\n",
    "                    for i in range(len(params)):\n",
    "                        params[i][...] = self.best_params[idx][i]\n",
    "                    idx += 1\n",
    "    \n",
    "    def compute_metrics(self, y_pred, y_true):\n",
    "        mse = np.mean((y_pred - y_true)**2)\n",
    "        return {'mse': mse}\n",
    "    \n",
    "    def train_epoch(self, X, y, batch_size=32, training=True):\n",
    "        epoch_loss = 0\n",
    "        epoch_metrics = {'mse': 0}\n",
    "        num_batches = int(np.ceil(len(X) / batch_size))\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X[start:end]\n",
    "            y_batch = y[start:end]\n",
    "            output = self.model.forward(X_batch)\n",
    "            loss = self.criterion.updateOutput(output, y_batch)\n",
    "            metrics = self.compute_metrics(output, y_batch)\n",
    "            epoch_loss += loss * len(X_batch)\n",
    "            for k in epoch_metrics:\n",
    "                epoch_metrics[k] += metrics[k] * len(X_batch)\n",
    "            if training:\n",
    "                grad_output = self.criterion.updateGradInput(output, y_batch)\n",
    "                self.model.backward(X_batch, grad_output)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "        \n",
    "        return epoch_loss, epoch_metrics\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_metrics = self.train_epoch(X_train, y_train, batch_size, training=True)\n",
    "            self.train_history['loss'].append(train_loss)\n",
    "            self.train_history['mse'].append(train_metrics['mse'])\n",
    "            \n",
    "            val_loss, val_metrics = self.train_epoch(X_val, y_val, batch_size, training=False)\n",
    "            self.val_history['loss'].append(val_loss)\n",
    "            self.val_history['mse'].append(val_metrics['mse'])\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(epoch, -val_loss)\n",
    "            \n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "                self._save_best_params()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"LR: {self.optimizer.lr if hasattr(self.optimizer, 'lr') else 'N/A':.6f}\")\n",
    "        \n",
    "        self._load_best_params()\n",
    "        print(f\"Best model restored from epoch {self.best_epoch + 1}\")\n",
    "        \n",
    "        self.plot_history()\n",
    "        return self.train_history, self.val_history\n",
    "    \n",
    "    def plot_history(self):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(np.log(self.train_history['loss']), label='Train Loss')\n",
    "        plt.plot(np.log(self.val_history['loss']), label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss (logarithmized)')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(np.log(self.train_history['mse']), label='Train MSE')\n",
    "        plt.plot(np.log(self.val_history['mse']), label='Validation MSE')\n",
    "        plt.title('Training and Validation MSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE (logarithmized)')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: Train Loss: 204002222.3655 | Val Loss: 781172.0172 | LR: 0.010000\n",
      "Epoch 2/200: Train Loss: 1240232.3044 | Val Loss: 230783.5432 | LR: 0.010000\n",
      "Epoch 3/200: Train Loss: 386356.0021 | Val Loss: 96355.6510 | LR: 0.010000\n",
      "Epoch 4/200: Train Loss: 191030.9010 | Val Loss: 48291.1327 | LR: 0.010000\n",
      "Epoch 5/200: Train Loss: 117709.1236 | Val Loss: 16998.4907 | LR: 0.010000\n",
      "Epoch 6/200: Train Loss: 211994.2644 | Val Loss: 14783.2303 | LR: 0.010000\n",
      "Epoch 7/200: Train Loss: 276942.6787 | Val Loss: 9400.1065 | LR: 0.010000\n",
      "Epoch 8/200: Train Loss: 285278.5570 | Val Loss: 9133.5196 | LR: 0.010000\n",
      "Epoch 9/200: Train Loss: 208024.8166 | Val Loss: 75143.2804 | LR: 0.010000\n",
      "Epoch 10/200: Train Loss: 308348.3951 | Val Loss: 27707.9754 | LR: 0.010000\n",
      "Epoch 11/200: Train Loss: 352332.9875 | Val Loss: 6519.8640 | LR: 0.010000\n",
      "Epoch 12/200: Train Loss: 19412.8152 | Val Loss: 8758.9335 | LR: 0.010000\n",
      "Epoch 13/200: Train Loss: 1023204.4019 | Val Loss: 15213.8092 | LR: 0.010000\n",
      "Epoch 14/200: Train Loss: 56318.0642 | Val Loss: 2753.6250 | LR: 0.010000\n",
      "Epoch 15/200: Train Loss: 15563.8076 | Val Loss: 7170.4057 | LR: 0.010000\n",
      "Epoch 16/200: Train Loss: 267777.8465 | Val Loss: 13587.7277 | LR: 0.010000\n",
      "Epoch 17/200: Train Loss: 358516.8489 | Val Loss: 24340.8588 | LR: 0.010000\n",
      "Epoch 18/200: Train Loss: 35766.4174 | Val Loss: 6898.2937 | LR: 0.010000\n",
      "Epoch 19/200: Train Loss: 547867.5701 | Val Loss: 9250.7563 | LR: 0.005000\n",
      "Epoch 20/200: Train Loss: 6878.8157 | Val Loss: 1295.5263 | LR: 0.005000\n",
      "Epoch 21/200: Train Loss: 3357.4508 | Val Loss: 994.3320 | LR: 0.005000\n",
      "Epoch 22/200: Train Loss: 4952.2886 | Val Loss: 1537.1622 | LR: 0.005000\n",
      "Epoch 23/200: Train Loss: 15778.5158 | Val Loss: 10216.7577 | LR: 0.005000\n",
      "Epoch 24/200: Train Loss: 188739.8302 | Val Loss: 2356.4598 | LR: 0.005000\n",
      "Epoch 25/200: Train Loss: 7026.0636 | Val Loss: 1520.0208 | LR: 0.005000\n",
      "Epoch 26/200: Train Loss: 13993.6633 | Val Loss: 10746.0046 | LR: 0.002500\n",
      "Epoch 27/200: Train Loss: 2753.7558 | Val Loss: 630.1175 | LR: 0.002500\n",
      "Epoch 28/200: Train Loss: 3604.2963 | Val Loss: 972.6769 | LR: 0.002500\n",
      "Epoch 29/200: Train Loss: 13724.3079 | Val Loss: 3233.7417 | LR: 0.002500\n",
      "Epoch 30/200: Train Loss: 11303.7426 | Val Loss: 5750.5704 | LR: 0.002500\n",
      "Epoch 31/200: Train Loss: 27018.4925 | Val Loss: 1154.7605 | LR: 0.002500\n",
      "Epoch 32/200: Train Loss: 4333.9062 | Val Loss: 1849.0858 | LR: 0.001250\n",
      "Epoch 33/200: Train Loss: 1688.9330 | Val Loss: 522.1238 | LR: 0.001250\n",
      "Epoch 34/200: Train Loss: 2360.5447 | Val Loss: 759.5611 | LR: 0.001250\n",
      "Epoch 35/200: Train Loss: 2930.9687 | Val Loss: 856.9409 | LR: 0.001250\n",
      "Epoch 36/200: Train Loss: 4601.4283 | Val Loss: 2833.3907 | LR: 0.001250\n",
      "Epoch 37/200: Train Loss: 5116.1117 | Val Loss: 894.5488 | LR: 0.001250\n",
      "Epoch 38/200: Train Loss: 5161.4892 | Val Loss: 1154.0150 | LR: 0.000625\n",
      "Epoch 39/200: Train Loss: 1291.6587 | Val Loss: 470.8796 | LR: 0.000625\n",
      "Epoch 40/200: Train Loss: 1559.7407 | Val Loss: 480.5898 | LR: 0.000625\n",
      "Epoch 41/200: Train Loss: 2224.8964 | Val Loss: 510.9980 | LR: 0.000625\n",
      "Epoch 42/200: Train Loss: 2148.1303 | Val Loss: 561.9177 | LR: 0.000625\n",
      "Epoch 43/200: Train Loss: 2118.3064 | Val Loss: 630.8953 | LR: 0.000625\n",
      "Epoch 44/200: Train Loss: 2142.7801 | Val Loss: 661.8493 | LR: 0.000313\n",
      "Epoch 45/200: Train Loss: 1098.7223 | Val Loss: 356.5461 | LR: 0.000313\n",
      "Epoch 46/200: Train Loss: 1188.3456 | Val Loss: 434.6624 | LR: 0.000313\n",
      "Epoch 47/200: Train Loss: 1278.2914 | Val Loss: 445.6168 | LR: 0.000313\n",
      "Epoch 48/200: Train Loss: 1331.8015 | Val Loss: 454.5578 | LR: 0.000313\n",
      "Epoch 49/200: Train Loss: 1341.9302 | Val Loss: 458.0880 | LR: 0.000313\n",
      "Epoch 50/200: Train Loss: 1344.5839 | Val Loss: 459.7333 | LR: 0.000156\n",
      "Epoch 51/200: Train Loss: 954.9157 | Val Loss: 366.5257 | LR: 0.000156\n",
      "Epoch 52/200: Train Loss: 964.8281 | Val Loss: 345.9745 | LR: 0.000156\n",
      "Epoch 53/200: Train Loss: 986.6096 | Val Loss: 342.1564 | LR: 0.000156\n",
      "Epoch 54/200: Train Loss: 1000.5447 | Val Loss: 340.8856 | LR: 0.000156\n",
      "Epoch 55/200: Train Loss: 1006.9260 | Val Loss: 340.2718 | LR: 0.000156\n",
      "Epoch 56/200: Train Loss: 1009.0528 | Val Loss: 339.8000 | LR: 0.000156\n",
      "Epoch 57/200: Train Loss: 1009.3082 | Val Loss: 339.4581 | LR: 0.000156\n",
      "Epoch 58/200: Train Loss: 1009.2318 | Val Loss: 339.1891 | LR: 0.000156\n",
      "Epoch 59/200: Train Loss: 1008.8459 | Val Loss: 338.9594 | LR: 0.000156\n",
      "Epoch 60/200: Train Loss: 1008.5479 | Val Loss: 338.8165 | LR: 0.000156\n",
      "Epoch 61/200: Train Loss: 1008.1925 | Val Loss: 338.6087 | LR: 0.000156\n",
      "Epoch 62/200: Train Loss: 1007.7257 | Val Loss: 338.4001 | LR: 0.000156\n",
      "Epoch 63/200: Train Loss: 1007.3967 | Val Loss: 338.2586 | LR: 0.000156\n",
      "Epoch 64/200: Train Loss: 1007.0266 | Val Loss: 338.1354 | LR: 0.000156\n",
      "Epoch 65/200: Train Loss: 1006.6382 | Val Loss: 338.0602 | LR: 0.000156\n",
      "Epoch 66/200: Train Loss: 1006.2836 | Val Loss: 337.9439 | LR: 0.000156\n",
      "Epoch 67/200: Train Loss: 1005.8605 | Val Loss: 337.8711 | LR: 0.000156\n",
      "Epoch 68/200: Train Loss: 1005.5635 | Val Loss: 337.8152 | LR: 0.000156\n",
      "Epoch 69/200: Train Loss: 1005.1428 | Val Loss: 337.5851 | LR: 0.000156\n",
      "Epoch 70/200: Train Loss: 1004.8285 | Val Loss: 337.3611 | LR: 0.000156\n",
      "Epoch 71/200: Train Loss: 1004.4999 | Val Loss: 337.1779 | LR: 0.000156\n",
      "Epoch 72/200: Train Loss: 1004.1521 | Val Loss: 336.9949 | LR: 0.000156\n",
      "Epoch 73/200: Train Loss: 1003.8503 | Val Loss: 336.8335 | LR: 0.000156\n",
      "Epoch 74/200: Train Loss: 1003.6222 | Val Loss: 336.7490 | LR: 0.000156\n",
      "Epoch 75/200: Train Loss: 1003.2714 | Val Loss: 336.5296 | LR: 0.000156\n",
      "Epoch 76/200: Train Loss: 1003.0741 | Val Loss: 336.4139 | LR: 0.000156\n",
      "Epoch 77/200: Train Loss: 1002.7277 | Val Loss: 336.2300 | LR: 0.000156\n",
      "Epoch 78/200: Train Loss: 1002.6173 | Val Loss: 336.1598 | LR: 0.000156\n",
      "Epoch 79/200: Train Loss: 1002.2475 | Val Loss: 336.0367 | LR: 0.000156\n",
      "Epoch 80/200: Train Loss: 1002.0435 | Val Loss: 335.8937 | LR: 0.000156\n",
      "Epoch 81/200: Train Loss: 1001.7347 | Val Loss: 335.7832 | LR: 0.000156\n",
      "Epoch 82/200: Train Loss: 1001.4438 | Val Loss: 335.6992 | LR: 0.000156\n",
      "Epoch 83/200: Train Loss: 1001.3213 | Val Loss: 335.6220 | LR: 0.000156\n",
      "Epoch 84/200: Train Loss: 1001.0290 | Val Loss: 335.5041 | LR: 0.000156\n",
      "Epoch 85/200: Train Loss: 1000.8294 | Val Loss: 335.3659 | LR: 0.000156\n",
      "Epoch 86/200: Train Loss: 1000.5784 | Val Loss: 335.3018 | LR: 0.000156\n",
      "Epoch 87/200: Train Loss: 1000.3431 | Val Loss: 335.1898 | LR: 0.000156\n",
      "Epoch 88/200: Train Loss: 1000.1894 | Val Loss: 335.1843 | LR: 0.000156\n",
      "Epoch 89/200: Train Loss: 999.9499 | Val Loss: 335.0218 | LR: 0.000156\n",
      "Epoch 90/200: Train Loss: 999.6503 | Val Loss: 334.9551 | LR: 0.000156\n",
      "Epoch 91/200: Train Loss: 999.5384 | Val Loss: 334.8847 | LR: 0.000156\n",
      "Epoch 92/200: Train Loss: 999.3312 | Val Loss: 334.7170 | LR: 0.000156\n",
      "Epoch 93/200: Train Loss: 999.0991 | Val Loss: 334.7159 | LR: 0.000156\n",
      "Epoch 94/200: Train Loss: 998.8147 | Val Loss: 334.5991 | LR: 0.000156\n",
      "Epoch 95/200: Train Loss: 998.6674 | Val Loss: 334.4806 | LR: 0.000156\n",
      "Epoch 96/200: Train Loss: 998.4939 | Val Loss: 334.3483 | LR: 0.000156\n",
      "Epoch 97/200: Train Loss: 998.1991 | Val Loss: 334.2915 | LR: 0.000156\n",
      "Epoch 98/200: Train Loss: 998.2279 | Val Loss: 334.2158 | LR: 0.000156\n",
      "Epoch 99/200: Train Loss: 997.9751 | Val Loss: 334.1518 | LR: 0.000156\n",
      "Epoch 100/200: Train Loss: 997.7056 | Val Loss: 334.0751 | LR: 0.000156\n",
      "Epoch 101/200: Train Loss: 997.5929 | Val Loss: 333.9572 | LR: 0.000156\n",
      "Epoch 102/200: Train Loss: 997.4030 | Val Loss: 333.8517 | LR: 0.000156\n",
      "Epoch 103/200: Train Loss: 997.0428 | Val Loss: 333.6850 | LR: 0.000156\n",
      "Epoch 104/200: Train Loss: 996.8269 | Val Loss: 333.5568 | LR: 0.000156\n",
      "Epoch 105/200: Train Loss: 996.5035 | Val Loss: 333.3109 | LR: 0.000156\n",
      "Epoch 106/200: Train Loss: 996.1898 | Val Loss: 333.0460 | LR: 0.000156\n",
      "Epoch 107/200: Train Loss: 995.9711 | Val Loss: 332.9287 | LR: 0.000156\n",
      "Epoch 108/200: Train Loss: 995.6915 | Val Loss: 332.7856 | LR: 0.000156\n",
      "Epoch 109/200: Train Loss: 995.4005 | Val Loss: 332.5347 | LR: 0.000156\n",
      "Epoch 110/200: Train Loss: 995.1969 | Val Loss: 332.4020 | LR: 0.000156\n",
      "Epoch 111/200: Train Loss: 994.8642 | Val Loss: 332.2304 | LR: 0.000156\n",
      "Epoch 112/200: Train Loss: 994.6461 | Val Loss: 331.8906 | LR: 0.000156\n",
      "Epoch 113/200: Train Loss: 994.3874 | Val Loss: 331.7269 | LR: 0.000156\n",
      "Epoch 114/200: Train Loss: 994.1126 | Val Loss: 331.4156 | LR: 0.000156\n",
      "Epoch 115/200: Train Loss: 993.9427 | Val Loss: 331.0861 | LR: 0.000156\n",
      "Epoch 116/200: Train Loss: 993.5911 | Val Loss: 330.8766 | LR: 0.000156\n",
      "Epoch 117/200: Train Loss: 993.4346 | Val Loss: 330.6630 | LR: 0.000156\n",
      "Epoch 118/200: Train Loss: 993.2196 | Val Loss: 330.5187 | LR: 0.000156\n",
      "Epoch 119/200: Train Loss: 993.1079 | Val Loss: 330.3084 | LR: 0.000156\n",
      "Epoch 120/200: Train Loss: 992.7880 | Val Loss: 330.0718 | LR: 0.000156\n",
      "Epoch 121/200: Train Loss: 992.6204 | Val Loss: 329.8982 | LR: 0.000156\n",
      "Epoch 122/200: Train Loss: 992.4494 | Val Loss: 329.7504 | LR: 0.000156\n",
      "Epoch 123/200: Train Loss: 992.4677 | Val Loss: 329.5726 | LR: 0.000156\n",
      "Epoch 124/200: Train Loss: 992.0498 | Val Loss: 329.4183 | LR: 0.000156\n",
      "Epoch 125/200: Train Loss: 992.1039 | Val Loss: 329.2322 | LR: 0.000156\n",
      "Epoch 126/200: Train Loss: 991.9496 | Val Loss: 329.1521 | LR: 0.000156\n",
      "Epoch 127/200: Train Loss: 991.8862 | Val Loss: 328.9597 | LR: 0.000156\n",
      "Epoch 128/200: Train Loss: 991.6678 | Val Loss: 328.7892 | LR: 0.000156\n",
      "Epoch 129/200: Train Loss: 991.5350 | Val Loss: 328.5754 | LR: 0.000156\n",
      "Epoch 130/200: Train Loss: 991.2895 | Val Loss: 328.3994 | LR: 0.000156\n",
      "Epoch 131/200: Train Loss: 991.0737 | Val Loss: 328.2833 | LR: 0.000156\n",
      "Epoch 132/200: Train Loss: 990.9519 | Val Loss: 328.1249 | LR: 0.000156\n",
      "Epoch 133/200: Train Loss: 990.8220 | Val Loss: 328.0933 | LR: 0.000156\n",
      "Epoch 134/200: Train Loss: 990.6218 | Val Loss: 327.9099 | LR: 0.000156\n",
      "Epoch 135/200: Train Loss: 990.5632 | Val Loss: 327.8137 | LR: 0.000156\n",
      "Epoch 136/200: Train Loss: 990.4321 | Val Loss: 327.6311 | LR: 0.000156\n",
      "Epoch 137/200: Train Loss: 990.1631 | Val Loss: 327.5514 | LR: 0.000156\n",
      "Epoch 138/200: Train Loss: 990.1662 | Val Loss: 327.3534 | LR: 0.000156\n",
      "Epoch 139/200: Train Loss: 990.0091 | Val Loss: 327.2935 | LR: 0.000156\n",
      "Epoch 140/200: Train Loss: 989.8544 | Val Loss: 327.1401 | LR: 0.000156\n",
      "Epoch 141/200: Train Loss: 989.6592 | Val Loss: 327.0623 | LR: 0.000156\n",
      "Epoch 142/200: Train Loss: 989.6518 | Val Loss: 326.9593 | LR: 0.000156\n",
      "Epoch 143/200: Train Loss: 989.4196 | Val Loss: 326.7887 | LR: 0.000156\n",
      "Epoch 144/200: Train Loss: 989.4359 | Val Loss: 326.8003 | LR: 0.000156\n",
      "Epoch 145/200: Train Loss: 989.2387 | Val Loss: 326.6421 | LR: 0.000156\n",
      "Epoch 146/200: Train Loss: 989.1911 | Val Loss: 326.6322 | LR: 0.000156\n",
      "Epoch 147/200: Train Loss: 989.2286 | Val Loss: 326.4592 | LR: 0.000156\n",
      "Epoch 148/200: Train Loss: 988.9434 | Val Loss: 326.3856 | LR: 0.000156\n",
      "Epoch 149/200: Train Loss: 989.0223 | Val Loss: 326.3758 | LR: 0.000156\n",
      "Epoch 150/200: Train Loss: 988.8028 | Val Loss: 326.2129 | LR: 0.000156\n",
      "Epoch 151/200: Train Loss: 988.7591 | Val Loss: 326.1163 | LR: 0.000156\n",
      "Epoch 152/200: Train Loss: 988.6517 | Val Loss: 326.0505 | LR: 0.000156\n",
      "Epoch 153/200: Train Loss: 988.5084 | Val Loss: 325.9152 | LR: 0.000156\n",
      "Epoch 154/200: Train Loss: 988.5338 | Val Loss: 325.8182 | LR: 0.000156\n",
      "Epoch 155/200: Train Loss: 988.2453 | Val Loss: 325.7080 | LR: 0.000156\n",
      "Epoch 156/200: Train Loss: 988.1931 | Val Loss: 325.5618 | LR: 0.000156\n",
      "Epoch 157/200: Train Loss: 988.1815 | Val Loss: 325.5657 | LR: 0.000156\n",
      "Epoch 158/200: Train Loss: 988.1058 | Val Loss: 325.4697 | LR: 0.000156\n",
      "Epoch 159/200: Train Loss: 987.9453 | Val Loss: 325.3614 | LR: 0.000156\n",
      "Epoch 160/200: Train Loss: 987.9211 | Val Loss: 325.2745 | LR: 0.000156\n",
      "Epoch 161/200: Train Loss: 987.8909 | Val Loss: 325.2316 | LR: 0.000156\n",
      "Epoch 162/200: Train Loss: 987.7783 | Val Loss: 325.1048 | LR: 0.000156\n",
      "Epoch 163/200: Train Loss: 987.6331 | Val Loss: 325.0414 | LR: 0.000156\n",
      "Epoch 164/200: Train Loss: 987.7285 | Val Loss: 324.9613 | LR: 0.000156\n",
      "Epoch 165/200: Train Loss: 987.4692 | Val Loss: 324.8660 | LR: 0.000156\n",
      "Epoch 166/200: Train Loss: 987.3550 | Val Loss: 324.8711 | LR: 0.000156\n",
      "Epoch 167/200: Train Loss: 987.3683 | Val Loss: 324.8697 | LR: 0.000156\n",
      "Epoch 168/200: Train Loss: 987.3696 | Val Loss: 324.7807 | LR: 0.000156\n",
      "Epoch 169/200: Train Loss: 987.1344 | Val Loss: 324.8152 | LR: 0.000156\n",
      "Epoch 170/200: Train Loss: 987.1868 | Val Loss: 324.7321 | LR: 0.000156\n",
      "Epoch 171/200: Train Loss: 987.1325 | Val Loss: 324.7595 | LR: 0.000156\n",
      "Epoch 172/200: Train Loss: 986.8599 | Val Loss: 324.6836 | LR: 0.000156\n",
      "Epoch 173/200: Train Loss: 986.9012 | Val Loss: 324.6805 | LR: 0.000156\n",
      "Epoch 174/200: Train Loss: 986.7654 | Val Loss: 324.6033 | LR: 0.000156\n",
      "Epoch 175/200: Train Loss: 986.6684 | Val Loss: 324.5924 | LR: 0.000156\n",
      "Epoch 176/200: Train Loss: 986.5735 | Val Loss: 324.5322 | LR: 0.000156\n",
      "Epoch 177/200: Train Loss: 986.6958 | Val Loss: 324.5229 | LR: 0.000156\n",
      "Epoch 178/200: Train Loss: 986.4239 | Val Loss: 324.4625 | LR: 0.000156\n",
      "Epoch 179/200: Train Loss: 986.4565 | Val Loss: 324.3785 | LR: 0.000156\n",
      "Epoch 180/200: Train Loss: 986.3939 | Val Loss: 324.4051 | LR: 0.000156\n",
      "Epoch 181/200: Train Loss: 986.3780 | Val Loss: 324.3292 | LR: 0.000156\n",
      "Epoch 182/200: Train Loss: 986.2114 | Val Loss: 324.2457 | LR: 0.000156\n",
      "Epoch 183/200: Train Loss: 986.2387 | Val Loss: 324.2148 | LR: 0.000156\n",
      "Epoch 184/200: Train Loss: 986.1086 | Val Loss: 324.1790 | LR: 0.000156\n",
      "Epoch 185/200: Train Loss: 986.1299 | Val Loss: 324.1825 | LR: 0.000156\n",
      "Epoch 186/200: Train Loss: 986.0473 | Val Loss: 324.0702 | LR: 0.000156\n",
      "Epoch 187/200: Train Loss: 985.9247 | Val Loss: 324.1213 | LR: 0.000156\n",
      "Epoch 188/200: Train Loss: 985.8833 | Val Loss: 323.9475 | LR: 0.000156\n",
      "Epoch 189/200: Train Loss: 985.9085 | Val Loss: 323.9880 | LR: 0.000156\n",
      "Epoch 190/200: Train Loss: 985.8599 | Val Loss: 323.9494 | LR: 0.000156\n",
      "Epoch 191/200: Train Loss: 985.7463 | Val Loss: 323.8330 | LR: 0.000156\n",
      "Epoch 192/200: Train Loss: 985.6573 | Val Loss: 323.8579 | LR: 0.000156\n",
      "Epoch 193/200: Train Loss: 985.7449 | Val Loss: 323.6963 | LR: 0.000156\n",
      "Epoch 194/200: Train Loss: 985.6384 | Val Loss: 323.8340 | LR: 0.000156\n",
      "Epoch 195/200: Train Loss: 985.4531 | Val Loss: 323.6589 | LR: 0.000156\n",
      "Epoch 196/200: Train Loss: 985.4728 | Val Loss: 323.6261 | LR: 0.000156\n",
      "Epoch 197/200: Train Loss: 985.4333 | Val Loss: 323.5974 | LR: 0.000156\n",
      "Epoch 198/200: Train Loss: 985.3274 | Val Loss: 323.5622 | LR: 0.000156\n",
      "Epoch 199/200: Train Loss: 985.3321 | Val Loss: 323.5679 | LR: 0.000156\n",
      "Epoch 200/200: Train Loss: 985.2278 | Val Loss: 323.5639 | LR: 0.000156\n",
      "Best model restored from epoch 198\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10 #число фичей\n",
    "m = 4 #число таргетов\n",
    "\n",
    "X, y = make_regression(n_samples=30000, n_features=n, n_targets=m, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "# scaler_X = StandardScaler()\n",
    "# X_train = scaler_X.fit_transform(X_train)\n",
    "# X_val = scaler_X.transform(X_val)\n",
    "# scaler_y = StandardScaler()\n",
    "# y_train = scaler_y.fit_transform(y_train)\n",
    "# y_val = scaler_y.transform(y_val)\n",
    "\n",
    "model_small = Sequential()\n",
    "model_small.add(Linear(n, 64))\n",
    "model_small.add(ReLU())\n",
    "model_small.add(Linear(64, 32))\n",
    "model_small.add(ELU())\n",
    "model_small.add(Linear(32, m))\n",
    "\n",
    "model_med = Sequential()\n",
    "model_med.add(Linear(n, 64))\n",
    "model_med.add(BatchNormalization())\n",
    "model_med.add(ReLU())\n",
    "model_med.add(Dropout(0.5))\n",
    "model_med.add(Linear(64, 32))\n",
    "model_med.add(BatchNormalization())\n",
    "model_med.add(ELU())\n",
    "model_med.add(Linear(32, m))\n",
    "\n",
    "model_large = Sequential()\n",
    "model_large.add(Linear(n, 128))\n",
    "model_large.add(BatchNormalization())\n",
    "model_large.add(ReLU())\n",
    "model_large.add(Dropout(0.5))\n",
    "model_large.add(Linear(128, 128))\n",
    "model_large.add(BatchNormalization())\n",
    "model_large.add(ELU())\n",
    "model_large.add(Dropout(0.5))\n",
    "model_large.add(Linear(128, 64))\n",
    "model_large.add(BatchNormalization())\n",
    "model_large.add(Gelu())\n",
    "model_large.add(Dropout(0.5))\n",
    "model_large.add(Linear(64, m))\n",
    "\n",
    "model = model_small\n",
    "optimizer = AdamOptimizer(model, lr=0.01)\n",
    "criterion = MSECriterion()\n",
    "scheduler = ReduceLROnPlateau(optimizer, initial_lr=0.01, factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "trainer = MSERegressionTrainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    patience=10\n",
    ")\n",
    "train_history, val_history = trainer.train(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    epochs=200,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# def evaluate_model(X, y, model):\n",
    "#     model.evaluate()\n",
    "#     predictions = model.forward(X)\n",
    "#     mse = np.mean((predictions - y)**2)\n",
    "#     print(f\"Test MSE: {mse:.4f}\")\n",
    "#     return predictions\n",
    "# test_predictions = evaluate_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
